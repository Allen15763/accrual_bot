import os
import time
from pathlib import Path
from typing import Optional, Dict, List
from datetime import datetime
import pandas as pd

from accrual_bot.core.pipeline.base import PipelineStep, StepResult, StepStatus
from accrual_bot.core.pipeline.context import ProcessingContext
from accrual_bot.core.pipeline.steps.common import (
    StepMetadataBuilder,
    create_error_metadata
)
from accrual_bot.core.datasources import (
    DataSourceFactory,
    DataSourcePool
)


class SPXExportStep(PipelineStep):
    """
    SPX å°å‡ºæ­¥é©Ÿ
    
    åŠŸèƒ½: å°‡è™•ç†å®Œæˆçš„æ•¸æ“šå°å‡ºåˆ° Excel
    
    è¼¸å…¥: Processed DataFrame
    è¼¸å‡º: Excel file path
    """
    
    def __init__(self, 
                 name: str = "SPXExport",
                 output_dir: str = "output",
                 **kwargs):
        super().__init__(name, description="Export SPX processed data", **kwargs)
        self.output_dir = output_dir
    
    async def execute(self, context: ProcessingContext) -> StepResult:
        """åŸ·è¡Œå°å‡º"""
        start_time = time.time()
        try:
            df = context.data.copy()
            
            # æ¸…ç† <NA> å€¼
            df_export = df.replace('<NA>', pd.NA)
            
            # ç”Ÿæˆæ–‡ä»¶å
            processing_date = context.metadata.processing_date
            entity_type = context.metadata.entity_type
            timestamp = pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')
            
            filename = f"{entity_type}_PO_{processing_date}_processed_{timestamp}.xlsx"
            
            # å‰µå»ºè¼¸å‡ºç›®éŒ„
            if not os.path.exists(self.output_dir):
                os.makedirs(self.output_dir)
            
            output_path = os.path.join(self.output_dir, filename)
            
            # ç¢ºä¿æ–‡ä»¶åå”¯ä¸€
            counter = 1
            while os.path.exists(output_path):
                filename = f"{entity_type}_PO_{processing_date}_processed_{timestamp}_{counter}.xlsx"
                output_path = os.path.join(self.output_dir, filename)
                counter += 1
            
            # å°å‡º Excel
            # df_export.to_excel(output_path, index=False)
            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                df_export.to_excel(writer, sheet_name='PO', index=False)
                context.get_auxiliary_data('locker_non_discount').to_excel(writer, 
                                                                           sheet_name='locker_non_discount')
                context.get_auxiliary_data('locker_discount').to_excel(writer, 
                                                                       sheet_name='locker_discount')
                context.get_auxiliary_data('kiosk_data').to_excel(writer, 
                                                                  sheet_name='kiosk_data')
            
            self.logger.info(f"Data exported to: {output_path}")
            duration = time.time() - start_time
            
            return StepResult(
                step_name=self.name,
                status=StepStatus.SUCCESS,
                message=f"Exported to {output_path}",
                duration=duration,
                metadata={
                    'output_path': output_path,
                    'rows_exported': len(df_export),
                    'columns_exported': len(df_export.columns)
                }
            )
            
        except Exception as e:
            self.logger.error(f"Export failed: {str(e)}", exc_info=True)
            context.add_error(f"Export failed: {str(e)}")
            duration = time.time() - start_time
            return StepResult(
                step_name=self.name,
                status=StepStatus.FAILED,
                error=e,
                duration=duration,
                message=str(e)
            )
    
    async def validate_input(self, context: ProcessingContext) -> bool:
        """é©—è­‰è¼¸å…¥"""
        if context.data is None or context.data.empty:
            self.logger.error("No data to export")
            return False
        
        return True
    

class AccountingOPSExportingStep(PipelineStep):
    """
    æœƒè¨ˆèˆ‡ OPS åº•ç¨¿æ¯”å°çµæœè¼¸å‡ºæ­¥é©Ÿ
    ç”¨æ–¼å°‡æœƒè¨ˆå‰æœŸåº•ç¨¿ã€OPS é©—æ”¶æª”æ¡ˆåŠæ¯”å°çµæœè¼¸å‡ºåˆ° Excel
    
    åŠŸèƒ½ï¼š
    1. å¾ ProcessingContext å–å¾—ä¸‰ä»½è³‡æ–™
       - accounting_workpaper: æœƒè¨ˆå‰æœŸåº•ç¨¿
       - ops_validation: OPS é©—æ”¶æª”æ¡ˆåº•ç¨¿
       - validation_comparison: æ¯”å°çµæœ
    2. ä½¿ç”¨ datasources æ¨¡çµ„çš„ ExcelSource çµ±ä¸€å¯«å…¥
    3. è¼¸å‡ºåˆ°å–®ä¸€ Excel æª”æ¡ˆçš„å¤šå€‹ sheet
    4. è‡ªå‹•ç”Ÿæˆæª”æ¡ˆåç¨±ï¼ˆå«æ™‚é–“æˆ³ï¼‰ï¼Œé¿å…è¦†è“‹
    5. æä¾›è©³ç´°çš„åŸ·è¡Œ metadata
    
    ä½¿ç”¨ç¯„ä¾‹ï¼š
        step = AccountingOPSExportingStep(
            name="ExportResults",
            output_dir="output",
            filename_template="{entity}_{type}_{date}_{timestamp}.xlsx",
            sheet_names={
                'accounting_workpaper': 'acc_raw',
                'ops_validation': 'ops_raw',
                'validation_comparison': 'result'
            }
        )
    """
    
    # é è¨­çš„ sheet åç¨±å°æ‡‰
    DEFAULT_SHEET_NAMES = {
        'accounting_workpaper': 'acc_raw',
        'ops_validation': 'ops_raw',
        'validation_comparison': 'result'
    }
    
    def __init__(
        self,
        name: str = "AccountingOPSExporting",
        output_dir: str = "output",
        filename_template: str = "{entity}_{type}_{date}_{timestamp}.xlsx",
        sheet_names: Optional[Dict[str, str]] = None,
        include_index: bool = False,
        **kwargs
    ):
        """
        åˆå§‹åŒ–æ­¥é©Ÿ
        
        Args:
            name: æ­¥é©Ÿåç¨±
            output_dir: è¼¸å‡ºç›®éŒ„è·¯å¾‘
            filename_template: æª”æ¡ˆåç¨±æ¨¡æ¿ï¼Œæ”¯æ´æ ¼å¼åŒ–è®Šæ•¸ï¼š
                {entity}: å¯¦é«”é¡å‹ (SPX/MOB/SPT)
                {type}: è™•ç†é¡å‹ (PO/PR)
                {date}: è™•ç†æ—¥æœŸ (YYYYMM)
                {timestamp}: æ™‚é–“æˆ³ (YYYYMMDD_HHMMSS)
            sheet_names: è‡ªè¨‚ sheet åç¨±å°æ‡‰ï¼Œæ ¼å¼ï¼š
                {
                    'accounting_workpaper': 'æœƒè¨ˆåº•ç¨¿',
                    'ops_validation': 'OPSåº•ç¨¿',
                    'validation_comparison': 'æ¯”å°çµæœ'
                }
            include_index: æ˜¯å¦åŒ…å« DataFrame çš„ index
            **kwargs: å…¶ä»– PipelineStep åƒæ•¸
        """
        super().__init__(
            name,
            description="Export accounting and OPS validation results to Excel",
            **kwargs
        )
        
        self.output_dir = Path(output_dir)
        self.filename_template = filename_template
        self.sheet_names = sheet_names or self.DEFAULT_SHEET_NAMES
        self.include_index = include_index
        
        # æ•¸æ“šæºé€£æ¥æ± 
        self.pool = DataSourcePool()
    
    async def execute(self, context: ProcessingContext) -> StepResult:
        """åŸ·è¡Œè¼¸å‡º"""
        start_time = time.time()
        start_datetime = datetime.now()
        
        try:
            self.logger.info("Starting export of accounting and OPS validation results...")
            
            # éšæ®µ 1: å¾ context å–å¾—è³‡æ–™
            data_dict = self._get_data_from_context(context)
            
            # éšæ®µ 2: ç”Ÿæˆè¼¸å‡ºæª”æ¡ˆè·¯å¾‘
            output_path = self._generate_output_path(context)
            
            # éšæ®µ 3: ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
            self._ensure_output_directory()
            
            # éšæ®µ 4: ä½¿ç”¨ datasources æ¨¡çµ„å¯«å…¥ Excel
            sheets_written = await self._write_to_excel(output_path, data_dict)
            
            # è¨ˆç®—åŸ·è¡Œæ™‚é–“
            duration = time.time() - start_time
            end_datetime = datetime.now()
            
            # æ§‹å»º metadata
            total_rows = sum(len(df) for df in data_dict.values())
            
            metadata = (
                StepMetadataBuilder()
                .set_row_counts(total_rows, total_rows)
                .set_process_counts(processed=total_rows)
                .set_time_info(start_datetime, end_datetime)
                .add_custom('output_path', str(output_path))
                .add_custom('sheets_written', sheets_written)
                .add_custom('file_size_bytes', output_path.stat().st_size)
                .add_custom('accounting_rows', len(data_dict.get('accounting_workpaper', pd.DataFrame())))
                .add_custom('ops_rows', len(data_dict.get('ops_validation', pd.DataFrame())))
                .add_custom('comparison_rows', len(data_dict.get('validation_comparison', pd.DataFrame())))
                .build()
            )
            
            self.logger.info(
                f"Successfully exported {len(sheets_written)} sheets "
                f"to {output_path} in {duration:.2f}s"
            )
            
            # å„²å­˜è¼¸å‡ºè·¯å¾‘åˆ° contextï¼ˆä¾›å¾ŒçºŒæ­¥é©Ÿä½¿ç”¨ï¼‰
            context.set_variable('export_output_path', str(output_path))
            
            return StepResult(
                step_name=self.name,
                status=StepStatus.SUCCESS,
                message=(
                    f"Exported {len(sheets_written)} sheets to {output_path.name}\n"
                    f"Sheets: {', '.join(sheets_written)}"
                ),
                duration=duration,
                metadata=metadata
            )
            
        except Exception as e:
            duration = time.time() - start_time
            
            self.logger.error(f"Export failed: {str(e)}", exc_info=True)
            context.add_error(f"Export failed: {str(e)}")
            
            error_metadata = create_error_metadata(
                e, context, self.name,
                output_dir=str(self.output_dir),
                stage='export'
            )
            
            return StepResult(
                step_name=self.name,
                status=StepStatus.FAILED,
                error=e,
                message=f"Failed to export data: {str(e)}",
                duration=duration,
                metadata=error_metadata
            )
        
        finally:
            # æ¸…ç†è³‡æº
            await self._cleanup_resources()
    
    def _get_data_from_context(
        self,
        context: ProcessingContext
    ) -> Dict[str, pd.DataFrame]:
        """
        å¾ context å–å¾—æ‰€æœ‰éœ€è¦è¼¸å‡ºçš„è³‡æ–™
        
        Args:
            context: è™•ç†ä¸Šä¸‹æ–‡
            
        Returns:
            Dict[str, pd.DataFrame]: è³‡æ–™å­—å…¸
            
        Raises:
            ValueError: å¦‚æœç¼ºå°‘å¿…è¦è³‡æ–™
        """
        data_dict = {}
        missing_data = []
        
        # å®šç¾©éœ€è¦çš„è³‡æ–™åŠæ˜¯å¦ç‚ºå¿…è¦
        required_data = {
            'accounting_workpaper': True,   # å¿…è¦
            'ops_validation': True,          # å¿…è¦
            'validation_comparison': True    # å¿…è¦
        }
        
        for data_name, is_required in required_data.items():
            df = context.get_auxiliary_data(data_name)
            
            if df is None or df.empty:
                if is_required:
                    missing_data.append(data_name)
                    self.logger.error(f"Required data not found: {data_name}")
                else:
                    self.logger.warning(f"Optional data not found: {data_name}")
            else:
                data_dict[data_name] = df.copy()
                self.logger.info(f"Retrieved {data_name}: {df.shape}")
        
        if missing_data:
            raise ValueError(
                f"Missing required data in context: {', '.join(missing_data)}"
            )
        
        return data_dict
    
    def _generate_output_path(self, context: ProcessingContext) -> Path:
        """
        ç”Ÿæˆè¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        
        Args:
            context: è™•ç†ä¸Šä¸‹æ–‡
            
        Returns:
            Path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        """
        # æº–å‚™æ ¼å¼åŒ–è®Šæ•¸
        format_vars = {
            'entity': context.metadata.entity_type,
            'type': context.metadata.processing_type,
            'date': context.metadata.processing_date,
            'timestamp': datetime.now().strftime('%Y%m%d_%H%M%S')
        }
        
        # æ ¼å¼åŒ–æª”æ¡ˆåç¨±
        filename = self.filename_template.format(**format_vars)
        
        # ç”Ÿæˆå®Œæ•´è·¯å¾‘
        output_path = self.output_dir / filename
        
        # ç¢ºä¿æª”æ¡ˆåç¨±å”¯ä¸€ï¼ˆé¿å…è¦†è“‹ï¼‰
        counter = 1
        original_path = output_path
        while output_path.exists():
            stem = original_path.stem
            suffix = original_path.suffix
            filename = f"{stem}_{counter}{suffix}"
            output_path = self.output_dir / filename
            counter += 1
        
        self.logger.info(f"Generated output path: {output_path}")
        return output_path
    
    def _ensure_output_directory(self):
        """ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨"""
        if not self.output_dir.exists():
            self.output_dir.mkdir(parents=True, exist_ok=True)
            self.logger.info(f"Created output directory: {self.output_dir}")
    
    async def _write_to_excel(
        self,
        output_path: Path,
        data_dict: Dict[str, pd.DataFrame]
    ) -> List[str]:
        """
        ä½¿ç”¨ pd.ExcelWriter å¯«å…¥ Excelï¼ˆå¤š sheetï¼‰
        
        Args:
            output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
            data_dict: è³‡æ–™å­—å…¸
            
        Returns:
            List[str]: æˆåŠŸå¯«å…¥çš„ sheet åç¨±åˆ—è¡¨
        """
        sheets_written = []
        
        try:
            # ä½¿ç”¨ pd.ExcelWriter ç¢ºä¿å¤š sheet å¯«å…¥çš„æ­£ç¢ºæ€§
            # é€™è£¡æš«æ™‚ä¸ä½¿ç”¨ datasourcesï¼Œå› ç‚º ExcelSource çš„ write() 
            # æ–¹æ³•åœ¨å¤š sheet å¯«å…¥æ™‚å¯èƒ½ä¸å¤ éˆæ´»
            
            # æ–¹æ¡ˆ 1: ç›´æ¥ä½¿ç”¨ pd.ExcelWriterï¼ˆæ¨è–¦ï¼Œæ›´å¯é ï¼‰
            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                for data_name, df in data_dict.items():
                    sheet_name = self.sheet_names.get(data_name, data_name)
                    
                    # æ¸…ç† <NA> å€¼
                    df_export = df.replace('<NA>', pd.NA)
                    
                    # å¯«å…¥ sheet
                    df_export.to_excel(
                        writer,
                        sheet_name=sheet_name,
                        index=self.include_index
                    )
                    
                    sheets_written.append(sheet_name)
                    self.logger.info(
                        f"Wrote sheet '{sheet_name}': {len(df_export)} rows, "
                        f"{len(df_export.columns)} columns"
                    )
            
            # æ–¹æ¡ˆ 2: ä½¿ç”¨ datasourcesï¼ˆå¦‚æœéœ€è¦ï¼‰
            # æ³¨æ„ï¼šéœ€è¦ä¾åºå¯«å…¥å¤šå€‹ sheet
            # source = DataSourceFactory.create_from_file(
            #     str(output_path),
            #     engine='openpyxl'
            # )
            # self.pool.add_source('output_excel', source)
            # 
            # for i, (data_name, df) in enumerate(data_dict.items()):
            #     sheet_name = self.sheet_names.get(data_name, data_name)
            #     df_export = df.replace('<NA>', pd.NA)
            #     
            #     # ç¬¬ä¸€å€‹ sheet ä½¿ç”¨è¦†å¯«æ¨¡å¼ï¼Œå¾ŒçºŒä½¿ç”¨è¿½åŠ æ¨¡å¼
            #     mode = 'w' if i == 0 else 'a'
            #     
            #     success = await source.write(
            #         df_export,
            #         sheet_name=sheet_name,
            #         index=self.include_index,
            #         mode=mode,
            #         if_sheet_exists='replace'
            #     )
            #     
            #     if success:
            #         sheets_written.append(sheet_name)
            
            return sheets_written
            
        except Exception as e:
            self.logger.error(f"Error writing to Excel: {str(e)}")
            raise
    
    async def _cleanup_resources(self):
        """æ¸…ç†è³‡æº"""
        try:
            await self.pool.close_all()
            self.logger.debug("All data sources closed")
        except Exception as e:
            self.logger.error(f"Error during resource cleanup: {str(e)}")
    
    async def validate_input(self, context: ProcessingContext) -> bool:
        """
        é©—è­‰è¼¸å…¥
        
        Args:
            context: è™•ç†ä¸Šä¸‹æ–‡
            
        Returns:
            bool: é©—è­‰æ˜¯å¦é€šé
        """
        # æª¢æŸ¥å¿…è¦è³‡æ–™æ˜¯å¦å­˜åœ¨
        required_data = [
            'accounting_workpaper',
            'ops_validation',
            'validation_comparison'
        ]
        
        missing_data = []
        for data_name in required_data:
            if not context.has_auxiliary_data(data_name):
                missing_data.append(data_name)
                self.logger.error(f"Required data not found: {data_name}")
        
        if missing_data:
            context.add_error(
                f"Missing required data for export: {', '.join(missing_data)}"
            )
            return False
        
        # æª¢æŸ¥è¼¸å‡ºç›®éŒ„è·¯å¾‘æ˜¯å¦æœ‰æ•ˆ
        try:
            # å˜—è©¦å‰µå»ºç›®éŒ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if not self.output_dir.exists():
                self.output_dir.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            self.logger.error(f"Invalid output directory: {str(e)}")
            context.add_error(f"Invalid output directory: {str(e)}")
            return False
        
        return True
    
    async def rollback(self, context: ProcessingContext, error: Exception):
        """
        å›æ»¾æ“ä½œ
        
        Args:
            context: è™•ç†ä¸Šä¸‹æ–‡
            error: è§¸ç™¼å›æ»¾çš„éŒ¯èª¤
        """
        self.logger.warning(
            f"Rolling back export due to error: {str(error)}"
        )
        
        # æª¢æŸ¥æ˜¯å¦æœ‰éƒ¨åˆ†å¯«å…¥çš„æª”æ¡ˆï¼Œå¦‚æœæœ‰å‰‡å˜—è©¦åˆªé™¤
        output_path_str = context.get_variable('export_output_path')
        if output_path_str:
            output_path = Path(output_path_str)
            if output_path.exists():
                try:
                    output_path.unlink()
                    self.logger.info(f"Deleted partial output file: {output_path}")
                except Exception as e:
                    self.logger.error(
                        f"Failed to delete partial output file: {str(e)}"
                    )
        
        # æ¸…ç†è³‡æº
        await self._cleanup_resources()


class SPXPRExportStep(PipelineStep):
    """
    SPX PR å°å‡ºæ­¥é©Ÿ
    
    åŠŸèƒ½ï¼šå°‡è™•ç†å®Œæˆçš„ PR æ•¸æ“šå°å‡ºåˆ° Excel
    
    èˆ‡ SPXExportStep çš„å·®ç•°ï¼š
    - åªè¼¸å‡ºä¸»æ•¸æ“šï¼ˆcontext.dataï¼‰ï¼Œä¸è™•ç†è¼”åŠ©æ•¸æ“š
    - æª”æ¡ˆåç¨±æ ¼å¼ï¼š{entity_type}_PR_{processing_date}_processed_{timestamp}.xlsx
    - å–®ä¸€ sheet è¼¸å‡ºï¼ˆsheetåç¨±ï¼š'PR'ï¼‰
    - æ›´ç°¡åŒ–çš„é‚è¼¯ï¼Œå°ˆæ³¨æ–¼ PR æ•¸æ“š
    
    ä½¿ç”¨ç¯„ä¾‹ï¼š
        step = SPXPRExportStep(
            name="ExportPRData",
            output_dir="output",
            sheet_name="PR"
        )
    """
    
    def __init__(
        self,
        name: str = "SPXPRExport",
        output_dir: str = "output",
        sheet_name: str = "PR",
        include_index: bool = False,
        **kwargs
    ):
        """
        åˆå§‹åŒ– PR å°å‡ºæ­¥é©Ÿ
        
        Args:
            name: æ­¥é©Ÿåç¨±
            output_dir: è¼¸å‡ºç›®éŒ„è·¯å¾‘
            sheet_name: Excel sheet åç¨±
            include_index: æ˜¯å¦åŒ…å« DataFrame çš„ index
            **kwargs: å…¶ä»– PipelineStep åƒæ•¸
        """
        super().__init__(
            name,
            description="Export SPX PR processed data to Excel",
            **kwargs
        )
        
        self.output_dir = Path(output_dir)
        self.sheet_name = sheet_name
        self.include_index = include_index
    
    async def execute(self, context: ProcessingContext) -> StepResult:
        """åŸ·è¡Œ PR æ•¸æ“šå°å‡º"""
        start_time = time.time()
        start_datetime = datetime.now()
        
        try:
            self.logger.info("=" * 70)
            self.logger.info("ğŸ“¤ é–‹å§‹å°å‡º SPX PR è™•ç†çµæœ")
            self.logger.info("=" * 70)
            
            # éšæ®µ 1: ç²å–æ•¸æ“š
            df = context.data.copy()
            
            if df.empty:
                raise ValueError("ä¸»æ•¸æ“šç‚ºç©ºï¼Œç„¡æ³•å°å‡º")
            
            # éšæ®µ 2: æ¸…ç†æ•¸æ“š
            df_export = self._clean_data(df)
            
            # éšæ®µ 3: ç”Ÿæˆè¼¸å‡ºè·¯å¾‘
            output_path = self._generate_output_path(context)
            
            # éšæ®µ 4: ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
            self._ensure_output_directory()
            
            # éšæ®µ 5: å¯«å…¥ Excel
            self._write_to_excel(output_path, df_export)
            
            # è¨ˆç®—åŸ·è¡Œæ™‚é–“
            duration = time.time() - start_time
            end_datetime = datetime.now()
            
            # æ§‹å»º metadata
            metadata = (
                StepMetadataBuilder()
                .set_row_counts(len(df_export), len(df_export))
                .set_process_counts(processed=len(df_export))
                .set_time_info(start_datetime, end_datetime)
                .add_custom('output_path', str(output_path))
                .add_custom('file_size_bytes', output_path.stat().st_size)
                .add_custom('columns_exported', len(df_export.columns))
                .add_custom('sheet_name', self.sheet_name)
                .build()
            )
            
            self.logger.info("=" * 70)
            self.logger.info("âœ… PR æ•¸æ“šå°å‡ºå®Œæˆ")
            self.logger.info(f"ğŸ“ è¼¸å‡ºè·¯å¾‘ï¼š{output_path}")
            self.logger.info(f"ğŸ“Š å°å‡ºè¨˜éŒ„ï¼š{len(df_export):,} ç­†")
            self.logger.info(f"ğŸ“‹ æ¬„ä½æ•¸é‡ï¼š{len(df_export.columns)} å€‹")
            self.logger.info(f"â±ï¸  åŸ·è¡Œæ™‚é–“ï¼š{duration:.2f} ç§’")
            self.logger.info("=" * 70)
            
            # å„²å­˜è¼¸å‡ºè·¯å¾‘åˆ° contextï¼ˆä¾›å¾ŒçºŒä½¿ç”¨æˆ–åƒè€ƒï¼‰
            context.set_variable('pr_export_output_path', str(output_path))
            
            return StepResult(
                step_name=self.name,
                status=StepStatus.SUCCESS,
                message=f"æˆåŠŸå°å‡º {len(df_export):,} ç­† PR æ•¸æ“šåˆ° {output_path.name}",
                duration=duration,
                metadata=metadata
            )
            
        except Exception as e:
            duration = time.time() - start_time
            
            self.logger.error(f"âŒ PR æ•¸æ“šå°å‡ºå¤±æ•—ï¼š{str(e)}", exc_info=True)
            context.add_error(f"PR å°å‡ºå¤±æ•—ï¼š{str(e)}")
            
            error_metadata = create_error_metadata(
                e, context, self.name,
                output_dir=str(self.output_dir),
                stage='pr_export'
            )
            
            return StepResult(
                step_name=self.name,
                status=StepStatus.FAILED,
                error=e,
                message=f"å°å‡ºå¤±æ•—ï¼š{str(e)}",
                duration=duration,
                metadata=error_metadata
            )
    
    def _clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ¸…ç†æ•¸æ“šï¼Œæº–å‚™å°å‡º
        
        æ¸…ç†æ“ä½œï¼š
        1. æ›¿æ› <NA> ç‚º pandas NA
        2. ç§»é™¤å®Œå…¨ç©ºç™½çš„è¡Œï¼ˆå¯é¸ï¼‰
        
        Args:
            df: åŸå§‹æ•¸æ“š
            
        Returns:
            pd.DataFrame: æ¸…ç†å¾Œçš„æ•¸æ“š
        """
        self.logger.debug("æ¸…ç†å°å‡ºæ•¸æ“š...")
        
        # æ›¿æ› <NA> å€¼
        df_clean = df.replace('<NA>', pd.NA)
        
        # è¨˜éŒ„æ¸…ç†çµæœ
        self.logger.debug(f"  âœ“ æ•¸æ“šæ¸…ç†å®Œæˆï¼š{len(df_clean)} è¡Œ")
        
        return df_clean
    
    def _generate_output_path(self, context: ProcessingContext) -> Path:
        """
        ç”Ÿæˆè¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        
        æª”æ¡ˆå‘½åæ ¼å¼ï¼š{entity_type}_PR_{processing_date}_processed_{timestamp}.xlsx
        
        Args:
            context: è™•ç†ä¸Šä¸‹æ–‡
            
        Returns:
            Path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
        """
        # æº–å‚™æª”æ¡ˆåç¨±çµ„ä»¶
        entity_type = context.metadata.entity_type
        processing_date = context.metadata.processing_date
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ç”Ÿæˆæª”æ¡ˆåç¨±
        filename = f"{entity_type}_PR_{processing_date}_processed_{timestamp}.xlsx"
        
        # ç”Ÿæˆå®Œæ•´è·¯å¾‘
        output_path = self.output_dir / filename
        
        # ç¢ºä¿æª”æ¡ˆåç¨±å”¯ä¸€ï¼ˆé¿å…è¦†è“‹ï¼‰
        counter = 1
        original_path = output_path
        while output_path.exists():
            stem = original_path.stem
            suffix = original_path.suffix
            filename = f"{stem}_{counter}{suffix}"
            output_path = self.output_dir / filename
            counter += 1
            
            if counter > 100:  # å®‰å…¨é™åˆ¶
                raise RuntimeError("ç„¡æ³•ç”Ÿæˆå”¯ä¸€çš„æª”æ¡ˆåç¨±ï¼ˆå·²å˜—è©¦ 100 æ¬¡ï¼‰")
        
        self.logger.debug(f"ç”Ÿæˆè¼¸å‡ºè·¯å¾‘ï¼š{output_path}")
        return output_path
    
    def _ensure_output_directory(self):
        """ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨"""
        if not self.output_dir.exists():
            self.output_dir.mkdir(parents=True, exist_ok=True)
            self.logger.debug(f"å‰µå»ºè¼¸å‡ºç›®éŒ„ï¼š{self.output_dir}")
    
    def _write_to_excel(self, output_path: Path, df: pd.DataFrame):
        """
        å¯«å…¥ Excel æª”æ¡ˆ
        
        ä½¿ç”¨ pd.ExcelWriter ç¢ºä¿æ•¸æ“šæ­£ç¢ºå¯«å…¥
        
        Args:
            output_path: è¼¸å‡ºæª”æ¡ˆè·¯å¾‘
            df: è¦å¯«å…¥çš„æ•¸æ“š
        """
        self.logger.debug(f"å¯«å…¥ Excelï¼š{output_path}")
        
        try:
            with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
                df.to_excel(
                    writer,
                    sheet_name=self.sheet_name,
                    index=self.include_index
                )
            
            self.logger.debug(
                f"  âœ“ æˆåŠŸå¯«å…¥ sheet '{self.sheet_name}'ï¼š"
                f"{len(df)} è¡Œï¼Œ{len(df.columns)} åˆ—"
            )
            
        except Exception as e:
            self.logger.error(f"å¯«å…¥ Excel å¤±æ•—ï¼š{str(e)}")
            raise
    
    async def validate_input(self, context: ProcessingContext) -> bool:
        """
        é©—è­‰è¼¸å…¥æ•¸æ“š
        
        æª¢æŸ¥é …ç›®ï¼š
        1. ä¸»æ•¸æ“šä¸ç‚ºç©º
        2. è™•ç†é¡å‹ç‚º PR
        3. è¼¸å‡ºç›®éŒ„è·¯å¾‘æœ‰æ•ˆ
        
        Args:
            context: è™•ç†ä¸Šä¸‹æ–‡
            
        Returns:
            bool: é©—è­‰æ˜¯å¦é€šé
        """
        # æª¢æŸ¥ä¸»æ•¸æ“š
        if context.data is None or context.data.empty:
            self.logger.error("âŒ ä¸»æ•¸æ“šç‚ºç©ºï¼Œç„¡æ³•å°å‡º")
            context.add_error("ä¸»æ•¸æ“šç‚ºç©ºï¼Œç„¡æ³•å°å‡º")
            return False
        
        # æª¢æŸ¥è™•ç†é¡å‹ï¼ˆå¯é¸ï¼Œå–æ±ºæ–¼æ˜¯å¦éœ€è¦å¼·åˆ¶ PRï¼‰
        if context.metadata.processing_type != "PR":
            self.logger.warning(
                f"âš ï¸  è™•ç†é¡å‹ä¸æ˜¯ PRï¼ˆç•¶å‰ï¼š{context.metadata.processing_type}ï¼‰ï¼Œ"
                "ä½†ä»ç¹¼çºŒå°å‡º"
            )
        
        # æª¢æŸ¥è¼¸å‡ºç›®éŒ„è·¯å¾‘æ˜¯å¦æœ‰æ•ˆ
        try:
            # å˜—è©¦å‰µå»ºç›®éŒ„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if not self.output_dir.exists():
                self.output_dir.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            self.logger.error(f"âŒ ç„¡æ•ˆçš„è¼¸å‡ºç›®éŒ„ï¼š{str(e)}")
            context.add_error(f"ç„¡æ•ˆçš„è¼¸å‡ºç›®éŒ„ï¼š{str(e)}")
            return False
        
        self.logger.debug("âœ… è¼¸å…¥é©—è­‰é€šé")
        return True
    
    async def rollback(self, context: ProcessingContext, error: Exception):
        """
        å›æ»¾æ“ä½œ
        
        å¦‚æœå°å‡ºéç¨‹ä¸­ç”¢ç”Ÿéƒ¨åˆ†æª”æ¡ˆï¼Œå˜—è©¦åˆªé™¤
        
        Args:
            context: è™•ç†ä¸Šä¸‹æ–‡
            error: è§¸ç™¼å›æ»¾çš„éŒ¯èª¤
        """
        self.logger.warning(f"âš ï¸  å›æ»¾ PR å°å‡ºï¼š{str(error)}")
        
        # æª¢æŸ¥æ˜¯å¦æœ‰éƒ¨åˆ†å¯«å…¥çš„æª”æ¡ˆ
        output_path_str = context.get_variable('pr_export_output_path')
        if output_path_str:
            output_path = Path(output_path_str)
            if output_path.exists():
                try:
                    output_path.unlink()
                    self.logger.info(f"âœ“ å·²åˆªé™¤éƒ¨åˆ†è¼¸å‡ºæª”æ¡ˆï¼š{output_path}")
                except Exception as e:
                    self.logger.error(f"âŒ ç„¡æ³•åˆªé™¤éƒ¨åˆ†è¼¸å‡ºæª”æ¡ˆï¼š{str(e)}")
