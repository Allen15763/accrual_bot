"""
SPT æœƒè¨ˆæ¨™ç±¤æ¨™è¨˜æ­¥é©Ÿ

åŠŸèƒ½ï¼š
æ ¹æ“šé…ç½®æª”æ¡ˆä¸­çš„æ¥­å‹™è¦å‰‡æ¨™è¨˜ POç‹€æ…‹ å’Œ Remarked by FN

æ¥­å‹™é‚è¼¯:
1. å„ªå…ˆç´šæ¢ä»¶ï¼šæ›´æ–° POç‹€æ…‹ å’Œ Remarked by FNï¼ˆå¼·åˆ¶è¦†è“‹ï¼‰
2. ERMæ¢ä»¶ï¼šåƒ…æ›´æ–° Remarked by FNï¼ˆä¸æ›´æ–°ç‹€æ…‹ï¼Œä¼°è¨ˆèˆ‡å¦ç”±ERMæ±ºå®šï¼‰

é…ç½®ä¾†æº:
- [spt_status_label_rules.priority_conditions]: å„ªå…ˆæ–¼ERMçš„æ¢ä»¶
- [spt_status_label_rules.erm_conditions]: ERMæ¢ä»¶
"""

import time
from typing import Dict, Any, List
from datetime import datetime
import pandas as pd
import numpy as np

from accrual_bot.core.pipeline.base import PipelineStep, StepResult, StepStatus
from accrual_bot.core.pipeline.context import ProcessingContext
from accrual_bot.core.pipeline.steps.common import StepMetadataBuilder, create_error_metadata
from accrual_bot.utils.config import config_manager


class SPTStatusLabelStep(PipelineStep):
    """
    æœƒè¨ˆæ¨™ç±¤æ¨™è¨˜æ­¥é©Ÿ (é…ç½®é©…å‹•)

    æ¥­å‹™é‚è¼¯:
    1. æ ¹æ“šé…ç½®æª”æ¡ˆä¸­çš„è¦å‰‡æ¨™è¨˜æœƒè¨ˆæ¨™ç±¤
    2. å„ªå…ˆç´šæ¢ä»¶ï¼šæ›´æ–° POç‹€æ…‹ å’Œ Remarked by FNï¼ˆå¼·åˆ¶è¦†è“‹ï¼‰
    3. ERMæ¢ä»¶ï¼šåƒ…æ›´æ–° Remarked by FNï¼ˆä¸æ›´æ–°ç‹€æ…‹ï¼Œä¼°è¨ˆèˆ‡å¦ç”±ERMæ±ºå®šï¼‰

    é…ç½®ä¾†æº:
    - [spt_status_label_rules.priority_conditions]: å„ªå…ˆæ–¼ERMçš„æ¢ä»¶
    - [spt_status_label_rules.erm_conditions]: ERMæ¢ä»¶

    è¼¸å…¥:
    - DataFrame with required columns

    è¼¸å‡º:
    - DataFrame with updated labels
    """

    def __init__(self,
                 name: str = "Accounting_Label_Marking",
                 status_column: str = "POç‹€æ…‹",
                 remark_column: str = "Remarked by FN",
                 **kwargs):
        """
        åˆå§‹åŒ–æœƒè¨ˆæ¨™ç±¤æ¨™è¨˜æ­¥é©Ÿ

        Args:
            name: æ­¥é©Ÿåç¨±
            status_column: ç‹€æ…‹æ¬„ä½åç¨±ï¼ˆé è¨­ç‚º POç‹€æ…‹ï¼‰
            remark_column: å‚™è¨»æ¬„ä½åç¨±ï¼ˆé è¨­ç‚º Remarked by FNï¼‰
        """
        super().__init__(
            name=name,
            description="Mark accounting labels based on business rules",
            **kwargs
        )
        self.status_column = status_column
        self.remark_column = remark_column

        # å¾é…ç½®æª”æ¡ˆè®€å–è¦å‰‡
        self.priority_rules = self._load_rules('priority_conditions')
        self.erm_rules = self._load_rules('erm_conditions')

        self.logger.info(f"å·²è¼‰å…¥ {len(self.priority_rules)} å€‹å„ªå…ˆç´šè¦å‰‡")
        self.logger.info(f"å·²è¼‰å…¥ {len(self.erm_rules)} å€‹ ERM è¦å‰‡")

    def _load_rules(self, rule_type: str) -> Dict[str, Dict[str, Any]]:
        """
        å¾é…ç½®æª”æ¡ˆè¼‰å…¥è¦å‰‡
        
        Args:
            rule_type: è¦å‰‡é¡å‹ ('priority_conditions' æˆ– 'erm_conditions')
            
        Returns:
            Dict[str, Dict]: è¦å‰‡å­—å…¸ï¼Œkey ç‚ºè¦å‰‡åç¨±
        """
        try:
            rules_config = config_manager._config_toml.get(
                'spt_status_label_rules', {}
            ).get(rule_type, {})

            if not rules_config:
                self.logger.warning(
                    f"æœªæ‰¾åˆ° spt_status_label_rules.{rule_type} é…ç½®ï¼Œå°‡ä½¿ç”¨ç©ºè¦å‰‡åˆ—è¡¨"
                )
                return {}

            self.logger.info(f"æˆåŠŸè¼‰å…¥ {len(rules_config)} æ¢ {rule_type} è¦å‰‡")

            # é©—è­‰è¦å‰‡
            exception_rules = ['exceed_period_but_pq_confirmed', 
                               'check_qty_and_pq_confirmed',
                               'parsing_err_but_pq_confirmed',
                               'incomplete_but_pq_confirmed',
                               'hris_bug']
            for rule_name, rule in rules_config.items():
                if 'remark' not in rule and rule_name not in exception_rules:
                    self.logger.warning(
                        f"è¦å‰‡ '{rule_name}' ç¼ºå°‘å¿…è¦æ¬„ä½ 'remark'"
                    )

            return dict(rules_config)

        except Exception as e:
            self.logger.error(f"è¼‰å…¥ {rule_type} è¦å‰‡å¤±æ•—: {str(e)}")
            return {}

    async def execute(self, context: ProcessingContext) -> StepResult:
        """åŸ·è¡Œæœƒè¨ˆæ¨™ç±¤æ¨™è¨˜é‚è¼¯"""
        start_time = time.time()
        start_datetime = datetime.now()

        try:
            df = context.data.copy()
            input_count = len(df)

            self.logger.info("=" * 60)
            self.logger.info("ğŸ·ï¸  é–‹å§‹åŸ·è¡Œæœƒè¨ˆæ¨™ç±¤æ¨™è¨˜...")
            self.logger.info(f"ğŸ“Š ç¸½è¨˜éŒ„æ•¸: {input_count:,}")
            self.logger.info("=" * 60)

            # å‹•æ…‹åˆ¤æ–·ç‹€æ…‹æ¬„ä½åç¨±
            self.status_column = self._get_status_column(df)

            # === éšæ®µ 1: æ‡‰ç”¨å„ªå…ˆç´šæ¢ä»¶ ===
            self.logger.info("âš¡ æ‡‰ç”¨å„ªå…ˆç´šæ¢ä»¶ï¼ˆå¼·åˆ¶è¦†è“‹ï¼‰...")
            priority_stats = self._apply_rules(
                df, self.priority_rules, update_status=True
            )

            # === éšæ®µ 2: æ‡‰ç”¨ ERM æ¢ä»¶ ===
            self.logger.info("ğŸ“‹ æ‡‰ç”¨ ERM æ¢ä»¶ï¼ˆåƒ…æ¨™è¨˜å‚™è¨»ï¼‰...")
            erm_stats = self._apply_rules(
                df, self.erm_rules, update_status=False
            )

            # === éšæ®µ 3: ç”Ÿæˆçµ±è¨ˆè³‡è¨Š ===
            total_labeled = sum(priority_stats.values()) + sum(erm_stats.values())

            statistics = {
                'total_records': input_count,
                'priority_labeled': sum(priority_stats.values()),
                'erm_labeled': sum(erm_stats.values()),
                'total_labeled': total_labeled,
                'label_rate': f"{(total_labeled / input_count * 100):.2f}%" if input_count > 0 else "0.00%",
                'priority_breakdown': priority_stats,
                'erm_breakdown': erm_stats
            }

            # === éšæ®µ 4: è¨˜éŒ„è©³ç´°æ—¥èªŒ ===
            self._log_detailed_statistics(statistics)

            # === éšæ®µ 5: æ›´æ–°ä¸Šä¸‹æ–‡ ===
            df = self._update_accrual_col(df)
            context.update_data(df)

            duration = time.time() - start_time
            end_datetime = datetime.now()

            self.logger.info("=" * 60)
            self.logger.info(f"âœ… æœƒè¨ˆæ¨™ç±¤æ¨™è¨˜å®Œæˆ (è€—æ™‚: {duration:.2f}ç§’)")
            self.logger.info("=" * 60)

            # æ§‹å»º metadata
            metadata = (StepMetadataBuilder()
                        .set_row_counts(input_count, len(df))
                        .set_process_counts(processed=total_labeled, skipped=input_count - total_labeled)
                        .set_time_info(start_datetime, end_datetime)
                        .add_custom('priority_labeled', sum(priority_stats.values()))
                        .add_custom('erm_labeled', sum(erm_stats.values()))
                        .add_custom('statistics', statistics)
                        .build())

            return StepResult(
                step_name=self.name,
                status=StepStatus.SUCCESS,
                data=df,
                message='-'.join([f"å·²æ¨™è¨˜ {total_labeled:,} ç­†è¨˜éŒ„\n",
                                 f"\t(å„ªå…ˆç´š: {sum(priority_stats.values()):,}, ERM: {sum(erm_stats.values()):,})"]),
                duration=duration,
                metadata=metadata
            )

        except Exception as e:
            duration = time.time() - start_time
            self.logger.error(f"âŒ æœƒè¨ˆæ¨™ç±¤æ¨™è¨˜å¤±æ•—: {str(e)}", exc_info=True)
            context.add_error(f"Accounting label marking failed: {str(e)}")

            error_metadata = create_error_metadata(
                e, context, self.name,
                stage='accounting_label_marking'
            )

            return StepResult(
                step_name=self.name,
                status=StepStatus.FAILED,
                error=e,
                message=f"æœƒè¨ˆæ¨™ç±¤æ¨™è¨˜å¤±æ•—: {str(e)}",
                duration=duration,
                metadata=error_metadata
            )

    def _get_status_column(self, df: pd.DataFrame) -> str:
        """å‹•æ…‹åˆ¤æ–·ç‹€æ…‹æ¬„ä½åç¨±"""
        if 'POç‹€æ…‹' in df.columns:
            return 'POç‹€æ…‹'
        elif 'PRç‹€æ…‹' in df.columns:
            return 'PRç‹€æ…‹'
        else:
            # å¦‚æœéƒ½ä¸å­˜åœ¨ï¼Œå‰µå»º POç‹€æ…‹ æ¬„ä½
            df['POç‹€æ…‹'] = pd.NA
            return 'POç‹€æ…‹'

    def _apply_rules(self, df: pd.DataFrame, rules: Dict[str, Dict[str, Any]],
                     update_status: bool) -> Dict[str, int]:
        """
        æ‡‰ç”¨è¦å‰‡å­—å…¸ï¼ˆé…ç½®é©…å‹•ï¼‰

        Args:
            df: DataFrame
            rules: è¦å‰‡å­—å…¸ {rule_name: rule_config}
            update_status: æ˜¯å¦æ›´æ–°ç‹€æ…‹æ¬„ä½

        Returns:
            Dict[str, int]: å„è¦å‰‡çš„åŒ¹é…è¨ˆæ•¸
        """
        stats = {}

        if not rules:
            self.logger.warning("æ²’æœ‰å¯ç”¨çš„è¦å‰‡")
            return stats

        for rule_name, rule in rules.items():
            status = rule.get('status')
            remark = rule.get('remark')
            matched_condition = rule.get('note')

            # æ§‹å»ºæ¢ä»¶
            condition = self._build_rule_condition(df, rule)

            # æ‡‰ç”¨è¦å‰‡
            count = condition.sum()
            if count > 0:
                # æ›´æ–°å‚™è¨»ï¼ˆç¸½æ˜¯æ›´æ–°ï¼‰
                df.loc[condition, self.remark_column] = remark
                df.loc[condition, 'matched_condition_on_status'] = matched_condition  # æš«æ™‚ä¸€ä½µæä¾›æ¢ä»¶è¨Šæ¯

                # æ›´æ–°ç‹€æ…‹ï¼ˆåƒ…å„ªå…ˆç´šæ¢ä»¶ï¼‰
                if update_status and status:
                    df.loc[condition, self.status_column] = status

                self.logger.debug(f"  âœ“ {rule_name}: {count:,} ç­†")
                stats[rule_name] = count

        return stats

    def _build_rule_condition(self, df: pd.DataFrame,
                              rule: Dict[str, Any]) -> pd.Series:
        """
        æ ¹æ“šè¦å‰‡é…ç½®æ§‹å»ºæ¢ä»¶

        æ”¯æ´çš„æ¢ä»¶é¡å‹ (å°æ‡‰ toml é…ç½®çš„ key):
        - keywords + field: é—œéµå­—åŒ¹é…ï¼ˆæŒ‡å®šæ¬„ä½ï¼‰
        - supplier: Supplier ç²¾ç¢ºåŒ¹é…
        - dept: Department ç²¾ç¢ºåŒ¹é…
        - dept_prefix: Department å‰ç¶´åŒ¹é…
        - dept_exclude_prefix: Department éå‰ç¶´åŒ¹é…
        - dept_include: Department åŒ…å«åŒ¹é… (regex)
        - dept_exclude: Department ä¸åŒ…å«åŒ¹é… (regex)
        - requester: Requester ç²¾ç¢ºåŒ¹é…
        - status_value_contains: ç‹€æ…‹æ¬„ä½regexåŒ¹é…
        - remarked_by_procurement: remarked_by_procurementå…§å®¹ç²¾ç¢ºåŒ¹é…

        Args:
            df: DataFrame
            rule: è¦å‰‡å­—å…¸

        Returns:
            pd.Series: å¸ƒæ—åºåˆ—è¡¨ç¤ºç¬¦åˆæ¢ä»¶çš„è¨˜éŒ„
        """
        # å¾å…¨éƒ¨è¨˜éŒ„é–‹å§‹
        condition = pd.Series([True] * len(df), index=df.index)

        # === é—œéµå­—æ¢ä»¶ (keywords + field) ===
        if 'keywords' in rule:
            keywords = rule['keywords']
            field = rule.get('field', 'Item Description')
            
            if field == 'Item Description':
                col_data = df.get('Item Description', pd.Series(dtype=str))
            else:
                col_data = df.get(field, pd.Series(dtype=str))
            
            keyword_condition = col_data.str.contains(keywords, na=False, regex=True)
            condition &= keyword_condition

        # === Supplier æ¢ä»¶ ===
        if 'supplier' in rule and rule['supplier']:
            supplier_col = self._get_column_by_pattern(df, r'(?i)supplier')
            if supplier_col:
                supplier = df.get(supplier_col, pd.Series(dtype=str))
                supplier_condition = supplier == rule['supplier']
                condition &= supplier_condition

        # === Department æ¢ä»¶ ===
        dept = df.get('Department', pd.Series(dtype=str))

        # ç²¾ç¢ºåŒ¹é…
        if 'dept' in rule and rule['dept']:
            dept_condition = dept == rule['dept']
            condition &= dept_condition

        # å‰ç¶´åŒ¹é…
        if 'dept_prefix' in rule and rule['dept_prefix']:
            prefix = rule['dept_prefix']
            dept_condition = dept.str.startswith(prefix, na=False)
            condition &= dept_condition

        # éå‰ç¶´åŒ¹é…
        if 'dept_exclude_prefix' in rule and rule['dept_exclude_prefix']:
            prefix = rule['dept_exclude_prefix']
            dept_condition = ~dept.str.startswith(prefix, na=False)
            condition &= dept_condition

        # åŒ…å«åŒ¹é… (regex)
        if 'dept_include' in rule and rule['dept_include']:
            pattern = rule['dept_include']
            dept_condition = dept.str.contains(pattern, na=False, regex=True)
            condition &= dept_condition

        # ä¸åŒ…å«åŒ¹é… (regex)
        if 'dept_exclude' in rule and rule['dept_exclude']:
            pattern = rule['dept_exclude']
            dept_condition = ~dept.str.contains(pattern, na=False, regex=True)
            condition &= dept_condition

        # === Requester æ¢ä»¶ ===
        if 'requester' in rule and rule['requester']:
            requester_col = self._get_column_by_pattern(df, r'(?i)requester')
            if requester_col:
                requester = df.get(requester_col, pd.Series(dtype=str))
                requester_condition = requester == rule['requester']
                condition &= requester_condition

        # ========== æ–°å¢ï¼šStatus æ¢ä»¶ (POç‹€æ…‹/PRç‹€æ…‹) ==========
        if 'status_value_contains' in rule and rule['status_value_contains']:
            # å‹•æ…‹åˆ¤æ–·ä½¿ç”¨å“ªå€‹ç‹€æ…‹æ¬„ä½ï¼ˆPOç‹€æ…‹ æˆ– PRç‹€æ…‹ï¼‰
            status_col = self.status_column  # å·²åœ¨ execute() ä¸­å‹•æ…‹è¨­å®š
            if status_col in df.columns:
                status_data = df.get(status_col, pd.Series(dtype=str))
                status_condition = status_data.str.contains(
                    rule['status_value_contains'], na=False, regex=True
                )
                condition &= status_condition

        # ========== æ–°å¢ï¼šRemarked by Procurement æ¢ä»¶ï¼ˆç²¾ç¢ºåŒ¹é…ï¼‰==========
        if 'remarked_by_procurement' in rule and rule['remarked_by_procurement']:
            procurement_col = self._get_column_by_pattern(df, r'(?i)remarked.*procurement')
            if procurement_col:
                procurement_data = df.get(procurement_col, pd.Series(dtype=str))
                procurement_condition = procurement_data == rule['remarked_by_procurement']
                condition &= procurement_condition

        return condition

    def _get_column_by_pattern(self, df: pd.DataFrame, pattern: str) -> str:
        """æ ¹æ“šæ­£å‰‡æ¨¡å¼ç²å–æ¬„ä½åç¨±"""
        matched_cols = df.filter(regex=pattern).columns
        return matched_cols[0] if len(matched_cols) > 0 else None

    def _log_detailed_statistics(self, stats: Dict[str, Any]):
        """è¨˜éŒ„è©³ç´°çµ±è¨ˆæ—¥èªŒ"""
        self.logger.info("\n" + "=" * 60)
        self.logger.info("ğŸ“Š æœƒè¨ˆæ¨™ç±¤æ¨™è¨˜çµ±è¨ˆå ±å‘Š")
        self.logger.info("=" * 60)
        self.logger.info(f"ğŸ“ˆ ç¸½è¨˜éŒ„æ•¸: {stats['total_records']:,}")
        self.logger.info(f"ğŸ·ï¸  å·²æ¨™è¨˜: {stats['total_labeled']:,} ({stats['label_rate']})")
        self.logger.info(f"   â€¢ å„ªå…ˆç´šæ¢ä»¶: {stats['priority_labeled']:,}")
        self.logger.info(f"   â€¢ ERM æ¢ä»¶: {stats['erm_labeled']:,}")

        if stats['priority_breakdown']:
            self.logger.info("\nğŸ“‹ å„ªå…ˆç´šæ¢ä»¶æ˜ç´°:")
            for label, count in sorted(stats['priority_breakdown'].items()):
                self.logger.info(f"   â€¢ {label}: {count:,}")

        if stats['erm_breakdown']:
            self.logger.info("\nğŸ“‹ ERM æ¢ä»¶æ˜ç´°:")
            for label, count in sorted(stats['erm_breakdown'].items()):
                self.logger.info(f"   â€¢ {label}: {count:,}")

        self.logger.info("=" * 60 + "\n")

    def _update_accrual_col(self, df: pd.DataFrame, accrual_col: str = 'æ˜¯å¦ä¼°è¨ˆå…¥å¸³') -> pd.DataFrame:
        df_copy = df.copy()
        df_copy[accrual_col] = np.where(
            df_copy[self.status_column].str.contains("å·²å®Œæˆ", na=False),
            'Y',
            df_copy[accrual_col]
        )
        return df_copy

    async def validate_input(self, context: ProcessingContext) -> bool:
        """
        é©—è­‰è¼¸å…¥è³‡æ–™çš„å®Œæ•´æ€§

        æª¢æŸ¥é …ç›®:
        1. DataFrame ä¸ç‚ºç©º
        2. å¿…è¦æ¬„ä½å­˜åœ¨
        3. é…ç½®è¦å‰‡å·²è¼‰å…¥
        """
        try:
            df = context.data

            # æª¢æŸ¥ DataFrame
            if df is None or df.empty:
                self.logger.error("âŒ è¼¸å…¥è³‡æ–™ç‚ºç©º")
                return False

            # æª¢æŸ¥å¿…è¦æ¬„ä½ï¼ˆåŸºæœ¬æ¬„ä½ï¼‰
            required_columns = ['Item Description', 'Department']
            missing_columns = [col for col in required_columns if col not in df.columns]
            if missing_columns:
                self.logger.error(f"âŒ ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_columns}")
                return False

            # æª¢æŸ¥ Supplier æ¬„ä½ï¼ˆæ”¯æ´ä¸åŒå‘½åï¼‰
            supplier_col = self._get_column_by_pattern(df, r'(?i)supplier')
            if not supplier_col:
                self.logger.error("âŒ ç¼ºå°‘ Supplier æ¬„ä½")
                return False

            # æª¢æŸ¥æˆ–å‰µå»ºå‚™è¨»æ¬„ä½
            if self.remark_column not in df.columns:
                self.logger.warning(f"âš ï¸  {self.remark_column} æ¬„ä½ä¸å­˜åœ¨ï¼Œå°‡è‡ªå‹•å‰µå»º")
                df[self.remark_column] = pd.NA

            # æª¢æŸ¥é…ç½®æ˜¯å¦è¼‰å…¥
            if not self.priority_rules and not self.erm_rules:
                self.logger.warning("âš ï¸  æœªè¼‰å…¥ä»»ä½•è¦å‰‡ï¼Œæ­¥é©Ÿå°‡ä¸æœƒé€²è¡Œä»»ä½•æ¨™è¨˜")

            self.logger.info("âœ… è¼¸å…¥é©—è­‰é€šé")
            return True

        except Exception as e:
            self.logger.error(f"âŒ é©—è­‰å¤±æ•—: {str(e)}", exc_info=True)
            return False

    async def rollback(self, context: ProcessingContext, error: Exception):
        """å›æ»¾æ“ä½œï¼ˆå¦‚éœ€è¦ï¼‰"""
        self.logger.warning(f"å›æ»¾æœƒè¨ˆæ¨™ç±¤æ¨™è¨˜ï¼š{str(error)}")
        # é€šå¸¸ä¸éœ€è¦ç‰¹æ®Šå›æ»¾æ“ä½œ
