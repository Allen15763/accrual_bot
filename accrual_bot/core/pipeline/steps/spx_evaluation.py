"""
é‚è¼¯åˆ¤æ–·ã€æ•¸æ“šè¨ˆç®—èˆ‡æ›´æ–°
"""
import time
import re
from dataclasses import dataclass
from typing import Optional, Dict, List, Tuple, Any, Union
from datetime import datetime
import pandas as pd
import numpy as np

from accrual_bot.core.pipeline.base import PipelineStep, StepResult, StepStatus
from accrual_bot.core.pipeline.context import ProcessingContext
from accrual_bot.utils.config import config_manager
from accrual_bot.core.pipeline.steps.common import StepMetadataBuilder


class StatusStage1Step(PipelineStep):
    """
    ç¬¬ä¸€éšæ®µç‹€æ…‹åˆ¤æ–·æ­¥é©Ÿï¼ˆæ··åˆæ¨¡å¼ï¼šé…ç½®é©…å‹• + ç¨‹å¼ç¢¼ä¿ç•™ï¼‰

    åŠŸèƒ½:
    æ ¹æ“šé—œå–®æ¸…å–®åŠé…ç½®è¦å‰‡çµ¦äºˆåˆå§‹ç‹€æ…‹

    é…ç½®é©…å‹•ï¼ˆå¾ stagging.toml [spx_status_stage1_rules] è®€å–ï¼‰ï¼š
    - æŠ¼é‡‘/ä¿è­‰é‡‘è­˜åˆ¥ã€BAOä¾›æ‡‰å•†GLèª¿æ•´ã€ä¸ŠæœˆFNå‚™è¨»é—œå–®
    - å…¬å…±è²»ç”¨ä¾›æ‡‰å•†ã€ç§Ÿé‡‘ç‹€æ…‹ã€Intermediaryç‹€æ…‹ã€è³‡ç”¢å¾…é©—æ”¶

    ç¨‹å¼ç¢¼ä¿ç•™ï¼ˆæ•¸æ“šé©…å‹•ï¼Œä¸é©åˆé…ç½®åŒ–ï¼‰ï¼š
    - é—œå–®æ¸…å–®æ¯”å°ï¼ˆå¾…é—œå–®/å·²é—œå–®ï¼‰
    - FAå‚™è¨»æå–ï¼ˆxxxxxxå…¥FAï¼‰
    - æ—¥æœŸæ ¼å¼è½‰æ›

    è¼¸å…¥: DataFrame + Closing list
    è¼¸å‡º: DataFrame with initial status
    """

    def __init__(self, name: str = "StatusStage1", **kwargs):
        super().__init__(name, description="Evaluate status stage 1", **kwargs)

        # åˆå§‹åŒ–é…ç½®é©…å‹•å¼•æ“
        from accrual_bot.core.pipeline.steps.spx_condition_engine import SPXConditionEngine
        self.engine = SPXConditionEngine('spx_status_stage1_rules')
    
    async def execute(self, context: ProcessingContext) -> StepResult:
        """åŸ·è¡Œç¬¬ä¸€éšæ®µç‹€æ…‹åˆ¤æ–·"""
        start_time = time.time()
        
        try:
            df = context.data.copy()
            df_spx_closing = context.get_auxiliary_data('closing_list')
            processing_date = context.metadata.processing_date
            
            self.logger.info("ğŸ”„ é–‹å§‹åŸ·è¡Œç¬¬ä¸€éšæ®µç‹€æ…‹åˆ¤æ–·...")
            
            # === éšæ®µ 1: é©—è­‰æ•¸æ“š ===
            if df_spx_closing is None or df_spx_closing.empty:
                self.logger.warning("âš ï¸  é—œå–®æ¸…å–®ç‚ºç©ºï¼Œè·³éç‹€æ…‹åˆ¤æ–·")
                return StepResult(
                    step_name=self.name,
                    status=StepStatus.SKIPPED,
                    data=df,
                    message="No closing list data"
                )
            
            self.logger.info(f"ğŸ“… è™•ç†æ—¥æœŸ: {processing_date}")
            self.logger.info(f"ğŸ“Š è¼¸å…¥è¨˜éŒ„æ•¸: {len(df):,}")
            self.logger.info(f"ğŸ“‹ é—œå–®æ¸…å–®è¨˜éŒ„æ•¸: {len(df_spx_closing):,}")
            
            # === éšæ®µ 2: çµ¦äºˆç‹€æ…‹æ¨™ç±¤ ===
            self.logger.info("ğŸ·ï¸  é–‹å§‹åˆ†é…ç‹€æ…‹æ¨™ç±¤...")
            df = self._give_status_stage_1(df, 
                                           df_spx_closing, 
                                           processing_date,
                                           entity_type=context.metadata.entity_type)
            
            # === éšæ®µ 3: ç”Ÿæˆæ‘˜è¦ ===
            tag_column = 'POç‹€æ…‹' if 'POç‹€æ…‹' in df.columns else 'PRç‹€æ…‹'
            summary = self._generate_label_summary(df, tag_column)
            
            # === éšæ®µ 4: è¨˜éŒ„æ‘˜è¦åˆ° Logger ===
            self._log_label_summary(summary, tag_column)
            
            # === éšæ®µ 5: æ›´æ–°ä¸Šä¸‹æ–‡ ===
            context.update_data(df)
            
            duration = time.time() - start_time
            
            self.logger.info(f"âœ… ç¬¬ä¸€éšæ®µç‹€æ…‹åˆ¤æ–·å®Œæˆ (è€—æ™‚: {duration:.2f}ç§’)")
            
            return StepResult(
                step_name=self.name,
                status=StepStatus.SUCCESS,
                data=df,
                message=f"ç‹€æ…‹æ¨™ç±¤åˆ†é…å®Œæˆ: {summary['labeled_count']} ç­†å·²æ¨™ç±¤",
                duration=duration,
                metadata=summary  # å°‡å®Œæ•´æ‘˜è¦æ”¾å…¥ metadata
            )
            
        except Exception as e:
            self.logger.error(f"âŒ ç¬¬ä¸€éšæ®µç‹€æ…‹åˆ¤æ–·å¤±æ•—: {str(e)}", exc_info=True)
            context.add_error(f"Status stage 1 evaluation failed: {str(e)}")
            duration = time.time() - start_time
            return StepResult(
                step_name=self.name,
                status=StepStatus.FAILED,
                error=e,
                duration=duration,
                message=str(e)
            )
    
    def _generate_label_summary(self, df: pd.DataFrame, 
                                tag_column: str) -> Dict[str, Any]:
        """
        ç”Ÿæˆæ¨™ç±¤åˆ†é…çš„è©³ç´°æ‘˜è¦
        
        çµ±è¨ˆå…§å®¹ï¼š
        1. å„æ¨™ç±¤æ•¸é‡èˆ‡ç™¾åˆ†æ¯”
        2. åˆ†é¡çµ±è¨ˆï¼ˆå·²å®Œæˆã€æœªå®Œæˆã€éŒ¯èª¤ç­‰ï¼‰
        3. éœ€è¦é—œæ³¨çš„ç•°å¸¸æ¨™ç±¤
        
        Args:
            df: è™•ç†å¾Œçš„ DataFrame
            tag_column: æ¨™ç±¤æ¬„ä½åç¨± ('POç‹€æ…‹' æˆ– 'PRç‹€æ…‹')
            
        Returns:
            Dict: åŒ…å«å®Œæ•´çµ±è¨ˆä¿¡æ¯çš„å­—å…¸
        """
        total_count = len(df)
        
        # æ¨™ç±¤åˆ†å¸ƒçµ±è¨ˆ
        label_counts = df[tag_column].value_counts().to_dict()
        label_percentages = (df[tag_column].value_counts(normalize=True) * 100).to_dict()
        
        # åˆ†é¡çµ±è¨ˆ
        completed_labels = ['å·²å®Œæˆ_ç§Ÿé‡‘', 'å·²å®Œæˆ_intermediary', 'å·²å…¥å¸³']
        incomplete_labels = ['æœªå®Œæˆ_ç§Ÿé‡‘', 'æœªå®Œæˆ_intermediary']
        pending_labels = ['å¾…é—œå–®', 'Pending_validating']
        closed_labels = ['å·²é—œå–®', 'åƒç…§ä¸Šæœˆé—œå–®']
        error_labels = [k for k in label_counts.keys() if 'error' in str(k).lower()]
        
        # æ§‹å»ºæ‘˜è¦
        summary = {
            'total_records': total_count,
            'labeled_count': df[tag_column].notna().sum(),
            'unlabeled_count': df[tag_column].isna().sum(),
            
            # æ¨™ç±¤åˆ†å¸ƒ
            'label_distribution': label_counts,
            'label_percentages': {k: round(v, 2) for k, v in label_percentages.items()},
            
            # åˆ†é¡çµ±è¨ˆ
            'category_stats': {
                'completed': sum(label_counts.get(label, 0) for label in completed_labels),
                'incomplete': sum(label_counts.get(label, 0) for label in incomplete_labels),
                'pending': sum(label_counts.get(label, 0) for label in pending_labels),
                'closed': sum(label_counts.get(label, 0) for label in closed_labels),
                'errors': sum(label_counts.get(label, 0) for label in error_labels),
            },
            
            # Top 5 æ¨™ç±¤
            'top_5_labels': dict(sorted(label_counts.items(), 
                                        key=lambda x: x[1], 
                                        reverse=True)[:5]),
        }
        
        return summary
    
    def _log_label_summary(self, summary: Dict[str, Any], tag_column: str):
        """
        ä»¥çµæ§‹åŒ–æ–¹å¼è¨˜éŒ„æ¨™ç±¤æ‘˜è¦åˆ° logger
        
        è¼¸å‡ºæ ¼å¼æ¸…æ™°æ˜“è®€ï¼Œä¾¿æ–¼ç›£æ§å’Œèª¿è©¦
        
        Args:
            summary: æ‘˜è¦çµ±è¨ˆæ•¸æ“š
            tag_column: æ¨™ç±¤æ¬„ä½åç¨±
        """
        self.logger.info("=" * 60)
        self.logger.info(f"ğŸ“Š {tag_column} æ¨™ç±¤åˆ†é…æ‘˜è¦")
        self.logger.info("=" * 60)
        
        # ç¸½è¦½çµ±è¨ˆ
        self.logger.info(f"ğŸ“ˆ ç¸½è¨˜éŒ„æ•¸: {summary['total_records']:,}")
        self.logger.info(f"   â”œâ”€ å·²æ¨™ç±¤: {summary['labeled_count']:,} "
                         f"({summary['labeled_count']/summary['total_records']*100:.1f}%)")
        self.logger.info(f"   â””â”€ æœªæ¨™ç±¤: {summary['unlabeled_count']:,}")
        
        # åˆ†é¡çµ±è¨ˆ
        self.logger.info("\nğŸ“‚ åˆ†é¡çµ±è¨ˆ:")
        category_stats = summary['category_stats']
        for category, count in category_stats.items():
            if count > 0:
                self.logger.info(f"   â€¢ {category:12s}: {count:5,} "
                                 f"({count/summary['total_records']*100:5.1f}%)")
        
        # Top 5 æ¨™ç±¤
        self.logger.info("\nğŸ† Top 5 æ¨™ç±¤:")
        for i, (label, count) in enumerate(summary['top_5_labels'].items(), 1):
            percentage = summary['label_percentages'].get(label, 0)
            self.logger.info(f"   {i}. {label:30s}: {count:5,} ({percentage:5.1f}%)")
        
        # ç•°å¸¸è­¦å‘Š
        if category_stats['errors'] > 0:
            self.logger.warning(f"\nâš ï¸  ç™¼ç¾ {category_stats['errors']} ç­†éŒ¯èª¤è¨˜éŒ„")
        
        self.logger.info("=" * 60)
    
    def _log_label_condition(self, condition_name: str, 
                             count: int, 
                             label: str):
        """
        è¨˜éŒ„å–®ä¸€æ¨™ç±¤æ¢ä»¶çš„çµæœ
        
        åƒè€ƒ SPXERMLogicStep._log_condition_result çš„é¢¨æ ¼
        
        Args:
            condition_name: æ¢ä»¶åç¨±
            count: ç¬¦åˆæ¢ä»¶çš„è¨˜éŒ„æ•¸
            label: è³¦äºˆçš„æ¨™ç±¤
        """
        if count > 0:
            self.logger.debug(f"âœ“ [{condition_name:30s}] â†’ '{label:20s}': {count:5,} ç­†")
    
    def _give_status_stage_1(self,
                             df: pd.DataFrame,
                             df_spx_closing: pd.DataFrame,
                             date,
                             **kwargs) -> pd.DataFrame:
        """çµ¦äºˆç¬¬ä¸€éšæ®µç‹€æ…‹ - æ··åˆæ¨¡å¼ï¼ˆé…ç½®é©…å‹• + ç¨‹å¼ç¢¼ä¿ç•™ï¼‰

        åŸ·è¡Œé †åºï¼š
        1. [ä»£ç¢¼] æ—¥æœŸæ ¼å¼è½‰æ›
        2. [ä»£ç¢¼] é—œå–®æ¸…å–®æ¯”å°ï¼ˆå¾…é—œå–®/å·²é—œå–®ï¼‰
        3. [ä»£ç¢¼] FAå‚™è¨»æå–ï¼ˆxxxxxxå…¥FAï¼‰
        4. [é…ç½®] å¼•æ“é©…å‹•çš„å¯é…ç½®æ¢ä»¶ï¼ˆæŠ¼é‡‘ã€GLèª¿æ•´ã€ç§Ÿé‡‘ã€è³‡ç”¢ç­‰ï¼‰

        Args:
            df: PO/PR DataFrame
            df_spx_closing: SPXé—œå–®æ•¸æ“šDataFrame
            date: è™•ç†æ—¥æœŸ (YYYYMM)

        Returns:
            pd.DataFrame: è™•ç†å¾Œçš„DataFrame
        """
        entity_type = kwargs.get('entity_type', 'SPX')
        is_po = 'POç‹€æ…‹' in df.columns
        tag_column = 'POç‹€æ…‹' if is_po else 'PRç‹€æ…‹'
        processing_type = 'PO' if is_po else 'PR'

        # === 1ï¼šæ—¥æœŸæ ¼å¼è½‰æ› ===
        df['Remarked by ä¸Šæœˆ FN'] = self.convert_date_format_in_remark(
            df['Remarked by ä¸Šæœˆ FN']
        )
        if 'Remarked by ä¸Šæœˆ FN PR' in df.columns:
            df['Remarked by ä¸Šæœˆ FN PR'] = self.convert_date_format_in_remark(
                df['Remarked by ä¸Šæœˆ FN PR']
            )

        # === 2ï¼šé—œå–®æ¸…å–®æ¯”å°ï¼ˆæ•¸æ“šé©…å‹•ï¼‰===
        c1, c2 = self.is_closed_spx(df_spx_closing)
        if is_po:
            id_col = 'PO#'
            closing_col = 'po_no'
        else:
            id_col = 'PR#'
            closing_col = 'new_pr_no'

        # å…ˆå–å¾—é—œå–®æ¸…å–®çš„po_no
        to_be_close = (df_spx_closing.loc[c1, closing_col].unique()
                       if c1.any() else [])
        closed = (df_spx_closing.loc[c2, closing_col].unique()
                  if c2.any() else [])
        
        # æŠŠè¦é—œå–®çš„è³‡æ–™åˆ†ç‚ºæ•´å¼µé—œè·Ÿéƒ¨åˆ†Itemé—œ
        to_be_close_all, to_be_close_partial = self._closing_by_line(df_spx_closing, to_be_close)
        closed_all, closed_partial = self._closing_by_line(df_spx_closing, closed)
        # åŠ ä¸Šå‰ç¶´
        to_be_close_all, to_be_close_partial = self._add_prefix(to_be_close_all), self._add_prefix(to_be_close_partial)
        closed_all, closed_partial = self._add_prefix(closed_all), self._add_prefix(closed_partial)

        # æ•´å¼µé—œ
        line_col = id_col.replace('#', ' Line')
        df = self._apply_closing_status(
            df, id_col, tag_column,
            to_be_close_all, 'å¾…é—œå–®', f'{id_col}åœ¨å¾…é—œå–®æ¸…å–®'
        )
        df = self._apply_closing_status(
            df, id_col, tag_column,
            closed_all, 'å·²é—œå–®', f'{id_col}åœ¨å·²é—œå–®æ¸…å–®'
        )
        # éƒ¨åˆ† Item é—œ
        df = self._apply_closing_status(
            df, line_col, tag_column,
            to_be_close_partial, 'å¾…é—œå–®', f'{line_col}åœ¨å¾…é—œå–®æ¸…å–®'
        )
        df = self._apply_closing_status(
            df, line_col, tag_column,
            closed_partial, 'å·²é—œå–®', f'{line_col}åœ¨å·²é—œå–®æ¸…å–®'
        )

        # === 3ï¼šFAå‚™è¨»æå–ï¼ˆéœ€ regex extractï¼‰===
        # PO: Remarked by ä¸Šæœˆ FN + Remarked by ä¸Šæœˆ FN PR
        # PR: Remarked by ä¸Šæœˆ FN
        fn_col = 'Remarked by ä¸Šæœˆ FN'
        has_fa = df[fn_col].astype('string').str.contains('å…¥FA', na=False)
        not_partial = ~df[fn_col].astype('string').str.contains('éƒ¨åˆ†å®Œæˆ', na=False)
        cond_fa_fn = has_fa & not_partial
        if cond_fa_fn.any():
            extracted = self.extract_fa_remark(df.loc[cond_fa_fn, fn_col])
            df.loc[cond_fa_fn, tag_column] = extracted
            self._log_label_condition(
                f'{processing_type}å‚™è¨»å…¥FA(FN)', cond_fa_fn.sum(), 'xxxxxxå…¥FA'
            )

        if is_po and 'Remarked by ä¸Šæœˆ FN PR' in df.columns:
            fn_pr_col = 'Remarked by ä¸Šæœˆ FN PR'
            has_fa_pr = df[fn_pr_col].astype('string').str.contains('å…¥FA', na=False)
            not_partial_pr = ~df[fn_pr_col].astype('string').str.contains('éƒ¨åˆ†å®Œæˆ', na=False)
            cond_fa_pr = has_fa_pr & not_partial_pr
            if cond_fa_pr.any():
                extracted_pr = self.extract_fa_remark(
                    df.loc[cond_fa_pr, fn_pr_col]
                )
                df.loc[cond_fa_pr, tag_column] = extracted_pr
                self._log_label_condition(
                    'PRå‚™è¨»å…¥FA', cond_fa_pr.sum(), 'xxxxxxå…¥FA'
                )

        # === é…ç½®é©…å‹•æ®µï¼šå¼•æ“è™•ç†å¯é…ç½®æ¢ä»¶ ===
        # å»ºç«‹ PO/PR æ¬„ä½åç¨±æ˜ å°„ï¼ˆå¼•æ“ config ä¸­ä½¿ç”¨é€šç”¨åç¨±ï¼‰
        supplier_col = 'PO Supplier' if is_po else 'PR Supplier'
        requester_col = 'PR Requester' if is_po else 'Requester'

        # å»ºç«‹æ¬„ä½åˆ¥åæ˜ å°„ï¼Œå¼•æ“ config ä¸­çš„ "Supplier" æ˜ å°„åˆ°å¯¦éš›æ¬„ä½
        if 'Supplier' not in df.columns and supplier_col in df.columns:
            df['Supplier'] = df[supplier_col]
        if 'Requester' not in df.columns and requester_col in df.columns:
            df['Requester'] = df[requester_col]

        engine_context = {
            'processing_date': date,
            'entity_type': entity_type,
            'prebuilt_masks': {},  # å¼•æ“æœƒè‡ªå‹•è¨ˆç®—å…§å»º mask
        }

        self.logger.info(
            f"ğŸ”„ å¼•æ“é©…å‹•: åŸ·è¡Œ {processing_type} é…ç½®åŒ–æ¢ä»¶..."
        )
        df, engine_stats = self.engine.apply_rules(
            df, tag_column, engine_context,
            processing_type=processing_type,
            update_no_status=True
        )

        # è¨˜éŒ„å¼•æ“çµ±è¨ˆ
        total_engine_hits = sum(engine_stats.values())
        self.logger.info(
            f"âœ… å¼•æ“é©…å‹•å®Œæˆ: {len(engine_stats)} æ¢è¦å‰‡, "
            f"å…±å‘½ä¸­ {total_engine_hits:,} ç­†"
        )

        # æ¸…ç†è‡¨æ™‚æ¬„ä½
        for temp_col in ['Supplier', 'Requester']:
            if temp_col in df.columns and temp_col not in [
                'PO Supplier', 'PR Supplier', 'PR Requester'
            ]:
                # åªæ¸…ç†æˆ‘å€‘æ·»åŠ çš„åˆ¥å
                actual_cols = [supplier_col, requester_col]
                if temp_col not in actual_cols:
                    df.drop(columns=[temp_col], inplace=True, errors='ignore')

        self.logger.info("æˆåŠŸçµ¦äºˆç¬¬ä¸€éšæ®µç‹€æ…‹")
        return df
    
    def is_closed_spx(self, df: pd.DataFrame) -> Tuple[pd.Series, pd.Series]:
        """åˆ¤æ–·SPXé—œå–®ç‹€æ…‹
        
        Args:
            df: é—œå–®æ•¸æ“šDataFrame
            
        Returns:
            Tuple[pd.Series, pd.Series]: (å¾…é—œå–®æ¢ä»¶, å·²é—œå–®æ¢ä»¶)
        """
        # [0]æœ‰æ–°çš„PRç·¨è™Ÿï¼Œä½†FNæœªä¸Šç³»çµ±é—œå–®çš„
        condition_to_be_closed = (
            (~df['new_pr_no'].isna()) & 
            (df['new_pr_no'] != '') & 
            (df['done_by_fn'].isna())
        )
        
        # [1]æœ‰æ–°çš„PRç·¨è™Ÿï¼Œä½†FNå·²ç¶“ä¸Šç³»çµ±é—œå–®çš„
        condition_closed = (
            (~df['new_pr_no'].isna()) & 
            (df['new_pr_no'] != '') & 
            (~df['done_by_fn'].isna())
        )
        
        return condition_to_be_closed, condition_closed
    
    def _closing_by_line(self, df: pd.DataFrame, po_no: List) -> List:

        remove_all = []
        remove_partial = []

        filtered_df = df[df['po_no'].isin(po_no)].copy()

        for index, row in filtered_df.iterrows():
            po = row['po_no']
            line = str(row['line_no']).strip()  # ç¢ºä¿è½‰æˆå­—ä¸²ä¸¦å»é™¤å‰å¾Œç©ºç™½
            
            # æƒ…æ³ 1ï¼šå¦‚æœæ˜¯ ALL
            if line == 'ALL':
                remove_all.append(po)
                
            # æƒ…æ³ 2ï¼šå¦‚æœæ˜¯ Line é–‹é ­çš„æŒ‡å®šè¡Œè™Ÿ
            elif line.startswith('Line'):
                # æ­¥é©Ÿ A: æŠŠ "Line" æ‹”æ‰ï¼Œåªç•™å¾Œé¢çš„æ•¸å­—å’Œç¬¦è™Ÿ
                num_str = line.replace('Line', '').strip()
                
                # æ­¥é©Ÿ B: ä½¿ç”¨æ­£å‰‡è¡¨é”å¼ï¼Œæ”¯æ´é “è™Ÿ (ã€) æˆ–æ˜¯åŠå½¢é€—è™Ÿ (,) åˆ‡å‰²
                parts = re.split(r'[ã€,]', num_str)
                
                # æ­¥é©Ÿ C: é‡å°åˆ‡å‰²å‡ºä¾†çš„æ¯ä¸€æ®µåšåˆ¤æ–·
                for part in parts:
                    part = part.strip()
                    
                    # å¦‚æœé€™æ®µè£¡é¢æœ‰æ³¢æµªè™Ÿ (ä»£è¡¨æ˜¯ç¯„åœï¼Œä¾‹å¦‚ 2~12)
                    if '~' in part:
                        start_str, end_str = part.split('~')
                        # ç¢ºä¿è£¡é¢çœŸçš„æ˜¯æ•¸å­—
                        if start_str.isdigit() and end_str.isdigit():
                            start_num = int(start_str)
                            end_num = int(end_str)
                            for i in range(start_num, end_num + 1):
                                remove_partial.append(f"{po}-{i}")
                    
                    # å¦‚æœé€™æ®µåªæ˜¯ç´”æ•¸å­— (ä»£è¡¨æ˜¯è·³è™Ÿçš„å–®ä¸€æ•¸å­—ï¼Œä¾‹å¦‚ 11)
                    elif part.isdigit():
                        remove_partial.append(f"{po}-{part}")
        return remove_all, remove_partial
    
    def _add_prefix(self, array: List) -> List:
        """æ–°å¢å‰ç¶´ä½¿å…¶ç¬¦åˆHRISç”¢å‡ºçš„PO#æ ¼å¼"""
        return ['SPTTW-' + i for i in array]

    def _apply_closing_status(self, df: pd.DataFrame,
                              match_col: str,
                              tag_column: str,
                              closing_list: List,
                              status: str,
                              label: str) -> pd.DataFrame:
        """æ¯”å°é—œå–®æ¸…å–®ä¸¦è³¦äºˆç‹€æ…‹æ¨™ç±¤

        Args:
            df: ä¸»è³‡æ–™ DataFrame
            match_col: ç”¨æ–¼æ¯”å°çš„æ¬„ä½åç¨±ï¼ˆå¦‚ 'PO#' æˆ– 'PO Line'ï¼‰
            tag_column: ç‹€æ…‹å¯«å…¥çš„ç›®æ¨™æ¬„ä½ï¼ˆ'POç‹€æ…‹' æˆ– 'PRç‹€æ…‹'ï¼‰
            closing_list: é—œå–®ç·¨è™Ÿæ¸…å–®
            status: è¦è³¦äºˆçš„ç‹€æ…‹å€¼ï¼ˆ'å¾…é—œå–®' æˆ– 'å·²é—œå–®'ï¼‰
            label: æ—¥èªŒæ¨™ç±¤æè¿°

        Returns:
            pd.DataFrame: æ›´æ–°å¾Œçš„ DataFrame
        """
        if not closing_list:
            return df
        mask = df[match_col].astype('string').isin(
            [str(x) for x in closing_list]
        )
        df.loc[mask, tag_column] = status
        self._log_label_condition(label, mask.sum(), status)
        return df

    def convert_date_format_in_remark(self, series: pd.Series) -> pd.Series:
        """è½‰æ›å‚™è¨»ä¸­çš„æ—¥æœŸæ ¼å¼ (YYYY/MM -> YYYYMM)
        
        Args:
            series: åŒ…å«æ—¥æœŸçš„Series
            
        Returns:
            pd.Series: è½‰æ›å¾Œçš„Series
        """
        try:
            return series.astype('string').str.replace(r'(\d{4})/(\d{2})', r'\1\2', regex=True)
        except Exception as e:
            self.logger.error(f"è½‰æ›æ—¥æœŸæ ¼å¼æ™‚å‡ºéŒ¯: {str(e)}", exc_info=True)
            return series
        
    def extract_fa_remark(self, series: pd.Series) -> pd.Series:
        """æå–FAå‚™è¨»ä¸­çš„æ—¥æœŸ
        
        Args:
            series: åŒ…å«FAå‚™è¨»çš„Series
            
        Returns:
            pd.Series: æå–çš„æ—¥æœŸSeries
        """
        try:
            return series.astype('string').str.extract(r'(\d{6}å…¥FA)', expand=False)
        except Exception as e:
            self.logger.error(f"æå–FAå‚™è¨»æ™‚å‡ºéŒ¯: {str(e)}", exc_info=True)
            return series
    
    async def validate_input(self, context: ProcessingContext) -> bool:
        """é©—è­‰è¼¸å…¥"""
        if context.data is None or context.data.empty:
            self.logger.error("No data for status stage 1")
            return False
        
        return True


@dataclass
class ERMConditions:
    """ERM åˆ¤æ–·æ¢ä»¶é›†åˆ - æé«˜å¯è®€æ€§"""
    # åŸºç¤æ¢ä»¶çµ„ä»¶
    no_status: pd.Series
    in_date_range: pd.Series
    erm_before_or_equal_file_date: pd.Series
    erm_after_file_date: pd.Series
    quantity_matched: pd.Series
    not_billed: pd.Series
    has_billing: pd.Series
    fully_billed: pd.Series
    has_unpaid_amount: pd.Series
    
    # å‚™è¨»æ¢ä»¶
    procurement_completed_or_rent: pd.Series
    fn_completed_or_posted: pd.Series
    pr_not_incomplete: pd.Series
    
    # FA æ¢ä»¶
    is_fa: pd.Series
    
    # éŒ¯èª¤æ¢ä»¶
    procurement_not_error: pd.Series
    out_of_date_range: pd.Series
    format_error: pd.Series


class SPXERMLogicStep(PipelineStep):
    """
    SPX ERM é‚è¼¯æ­¥é©Ÿ - é…ç½®é©…å‹•ç‰ˆæœ¬

    åŠŸèƒ½ï¼š
    1. è¨­ç½®æª”æ¡ˆæ—¥æœŸ
    2. åˆ¤æ–· 11 ç¨® PO/PR ç‹€æ…‹ï¼ˆå¾ [spx_erm_status_rules] é…ç½®è®€å–ï¼‰
    3. æ ¹æ“šç‹€æ…‹è¨­ç½®æ˜¯å¦ä¼°è¨ˆå…¥å¸³
    4. è¨­ç½®æœƒè¨ˆç›¸é—œæ¬„ä½ï¼ˆAccount code, Product code, Dep.ç­‰ï¼‰
    5. è¨ˆç®—é ä¼°é‡‘é¡ï¼ˆAccr. Amountï¼‰
    6. è™•ç†é ä»˜æ¬¾å’Œè² å‚µç§‘ç›®
    7. æª¢æŸ¥ PR Product Code

    æ¥­å‹™è¦å‰‡ï¼š
    - SPX é‚è¼¯ï¼šã€Œå·²å®Œæˆã€ç‹€æ…‹çš„é …ç›®éœ€è¦ä¼°åˆ—å…¥å¸³
    - å…¶ä»–ç‹€æ…‹ä¸€å¾‹ä¸ä¼°åˆ—ï¼ˆæ˜¯å¦ä¼°è¨ˆå…¥å¸³ = Nï¼‰
    - 11 å€‹ ERM æ¢ä»¶ç”±é…ç½®å¼•æ“ä¾ priority é †åºåŸ·è¡Œ

    è¼¸å…¥ï¼š
    - DataFrame with required columns
    - Reference data (ç§‘ç›®æ˜ å°„ã€è² å‚µç§‘ç›®)
    - Processing date

    è¼¸å‡ºï¼š
    - DataFrame with PO/PRç‹€æ…‹, æ˜¯å¦ä¼°è¨ˆå…¥å¸³, and accounting fields
    """

    def __init__(self, name: str = "SPX_ERM_Logic", **kwargs):
        super().__init__(
            name=name,
            description="Apply SPX ERM logic with 11 status conditions",
            **kwargs
        )

        # å¾é…ç½®è®€å–é—œéµåƒæ•¸
        self.fa_accounts = config_manager.get_list('SPX', 'fa_accounts', ['199999'])
        self.dept_accounts = config_manager.get_list('SPX', 'dept_accounts', [])

        # åˆå§‹åŒ–é…ç½®é©…å‹•å¼•æ“
        from accrual_bot.core.pipeline.steps.spx_condition_engine import SPXConditionEngine
        self.engine = SPXConditionEngine('spx_erm_status_rules')

        self.logger.info(f"Initialized {name} with FA accounts: {self.fa_accounts}")
    
    async def execute(self, context: ProcessingContext) -> StepResult:
        """åŸ·è¡Œ ERM é‚è¼¯"""
        start_time = time.time()
        try:
            df = context.data.copy()
            processing_date = context.get_variable('processing_date')
            
            # ç²å–åƒè€ƒæ•¸æ“š
            ref_account = context.get_auxiliary_data('reference_account')
            ref_liability = context.get_auxiliary_data('reference_liability')
            
            if ref_account is None or ref_liability is None:
                raise ValueError("ç¼ºå°‘åƒè€ƒæ•¸æ“šï¼šç§‘ç›®æ˜ å°„æˆ–è² å‚µç§‘ç›®")
            
            self.logger.info(f"é–‹å§‹ ERM é‚è¼¯è™•ç†ï¼Œè™•ç†æ—¥æœŸï¼š{processing_date}")
            
            # ========== éšæ®µ 1: è¨­ç½®åŸºæœ¬æ¬„ä½ ==========
            df = self._set_file_date(df, processing_date)
            
            # ========== éšæ®µ 2: æ§‹å»ºåˆ¤æ–·æ¢ä»¶ ==========
            status_column: str = self._get_status_column(df, context)
            conditions = self._build_conditions(df, processing_date, status_column)
            
            # ========== éšæ®µ 3: æ‡‰ç”¨ 11 å€‹ç‹€æ…‹æ¢ä»¶ ==========
            df = self._apply_status_conditions(df, conditions, status_column)
            
            # ========== éšæ®µ 4: è™•ç†æ ¼å¼éŒ¯èª¤ ==========
            df = self._handle_format_errors(df, conditions, status_column)
            
            # ========== éšæ®µ 5: è¨­ç½®æ˜¯å¦ä¼°è¨ˆå…¥å¸³ ==========
            df = self._set_accrual_flag(df, status_column)
            
            # ========== éšæ®µ 6: è¨­ç½®æœƒè¨ˆæ¬„ä½ ==========
            df = self._set_accounting_fields(df, ref_account, ref_liability)
            
            # ========== éšæ®µ 7: æª¢æŸ¥ PR Product Code ==========
            df = self._check_pr_product_code(df)
            
            # æ›´æ–°ä¸Šä¸‹æ–‡
            context.update_data(df)
            
            # ç”Ÿæˆçµ±è¨ˆè³‡è¨Š
            stats = self._generate_statistics(df, status_column)
            
            self.logger.info(
                f"ERM é‚è¼¯å®Œæˆ - "
                f"éœ€ä¼°åˆ—: {stats['accrual_count']} ç­†, "
                f"ç¸½è¨ˆ: {stats['total_count']} ç­†"
            )
            duration = time.time() - start_time
            
            return StepResult(
                step_name=self.name,
                status=StepStatus.SUCCESS,
                data=df,
                message=f"ERM é‚è¼¯å·²æ‡‰ç”¨ï¼Œ{stats['accrual_count']} ç­†éœ€ä¼°åˆ—",
                duration=duration,
                metadata=stats
            )
            
        except Exception as e:
            self.logger.error(f"ERM é‚è¼¯è™•ç†å¤±æ•—: {str(e)}", exc_info=True)
            context.add_error(f"ERM é‚è¼¯å¤±æ•—: {str(e)}")
            duration = time.time() - start_time
            return StepResult(
                step_name=self.name,
                status=StepStatus.FAILED,
                error=e,
                duration=duration,
                message=str(e)
            )
    
    # ========== éšæ®µ 1: åŸºæœ¬è¨­ç½® ==========
    
    def _set_file_date(self, df: pd.DataFrame, processing_date: int) -> pd.DataFrame:
        """è¨­ç½®æª”æ¡ˆæ—¥æœŸ"""
        df['æª”æ¡ˆæ—¥æœŸ'] = processing_date
        self.logger.debug(f"å·²è¨­ç½®æª”æ¡ˆæ—¥æœŸï¼š{processing_date}")
        return df
    
    def _get_status_column(self, df: pd.DataFrame, context: ProcessingContext) -> str:
        """å‹•æ…‹åˆ¤æ–·ç‹€æ…‹æ¬„ä½"""
        if 'POç‹€æ…‹' in df.columns:
            return 'POç‹€æ…‹'
        elif 'PRç‹€æ…‹' in df.columns:
            return 'PRç‹€æ…‹'
        else:
            # æ ¹æ“š context å‰µå»ºæ¬„ä½
            processing_type = context.metadata.processing_type
            return f"{processing_type}ç‹€æ…‹"
    
    # ========== éšæ®µ 2: æ§‹å»ºæ¢ä»¶ ==========
    
    def _build_conditions(self, df: pd.DataFrame, file_date: int,
                          status_column: str) -> ERMConditions:
        """
        æ§‹å»ºæ‰€æœ‰åˆ¤æ–·æ¢ä»¶
        
        å°‡æ¢ä»¶é‚è¼¯é›†ä¸­åœ¨æ­¤è™•ï¼Œæé«˜å¯è®€æ€§å’Œç¶­è­·æ€§
        """
        # åŸºç¤ç‹€æ…‹æ¢ä»¶
        no_status = (df[status_column].isna()) | (df[status_column] == 'nan')
        
        # æ—¥æœŸç¯„åœæ¢ä»¶
        ym_start = df['YMs of Item Description'].str[:6].astype('Int32')
        ym_end = df['YMs of Item Description'].str[7:].astype('Int32')
        erm = df['Expected Received Month_è½‰æ›æ ¼å¼']
        
        in_date_range = erm.between(ym_start, ym_end, inclusive='both')
        erm_before_or_equal_file_date = erm <= file_date
        erm_after_file_date = erm > file_date
        
        # æ•¸é‡æ¢ä»¶
        quantity_matched = df['Entry Quantity'] == df['Received Quantity']
        
        # å¸³å‹™æ¢ä»¶
        not_billed = df['Entry Billed Amount'].astype('Float64') == 0
        has_billing = df['Billed Quantity'] != '0'
        fully_billed = (
            df['Entry Amount'].astype('Float64') - 
            df['Entry Billed Amount'].astype('Float64')
        ) == 0
        has_unpaid_amount = (
            df['Entry Amount'].astype('Float64') - 
            df['Entry Billed Amount'].astype('Float64')
        ) != 0
        
        # å‚™è¨»æ¢ä»¶
        procurement_completed_or_rent = df['Remarked by Procurement'].str.contains(
            '(?i)å·²å®Œæˆ|rent', na=False
        )
        fn_completed_or_posted = df['Remarked by ä¸Šæœˆ FN'].str.contains(
            '(?i)å·²å®Œæˆ|å·²å…¥å¸³', na=False
        )
        pr_not_incomplete = ~df['Remarked by ä¸Šæœˆ FN PR'].str.contains(
            '(?i)æœªå®Œæˆ', na=False
        )
        
        # FA æ¢ä»¶
        is_fa = df['GL#'].astype('string').isin([str(x) for x in self.fa_accounts])
        
        # éŒ¯èª¤æ¢ä»¶
        procurement_not_error = df['Remarked by Procurement'] != 'error'
        out_of_date_range = (
            (in_date_range == False) & 
            (df['YMs of Item Description'] != '100001,100002')
        )
        format_error = df['YMs of Item Description'] == '100001,100002'
        
        return ERMConditions(
            no_status=no_status,
            in_date_range=in_date_range,
            erm_before_or_equal_file_date=erm_before_or_equal_file_date,
            erm_after_file_date=erm_after_file_date,
            quantity_matched=quantity_matched,
            not_billed=not_billed,
            has_billing=has_billing,
            fully_billed=fully_billed,
            has_unpaid_amount=has_unpaid_amount,
            procurement_completed_or_rent=procurement_completed_or_rent,
            fn_completed_or_posted=fn_completed_or_posted,
            pr_not_incomplete=pr_not_incomplete,
            is_fa=is_fa,
            procurement_not_error=procurement_not_error,
            out_of_date_range=out_of_date_range,
            format_error=format_error
        )
    
    # ========== éšæ®µ 3: æ‡‰ç”¨ç‹€æ…‹æ¢ä»¶ ==========
    
    def _apply_status_conditions(self, df: pd.DataFrame,
                                 cond: ERMConditions,
                                 status_column: str) -> pd.DataFrame:
        """
        æ‡‰ç”¨ ERM ç‹€æ…‹åˆ¤æ–·æ¢ä»¶ï¼ˆé…ç½®é©…å‹•ï¼‰

        å°‡é å…ˆè¨ˆç®—çš„ ERMConditions è½‰ç‚º prebuilt_masksï¼Œ
        ç”± SPXConditionEngine ä¾é…ç½®é †åºåŸ·è¡Œã€‚
        """
        # å°‡ ERMConditions è½‰ç‚ºå¼•æ“çš„ prebuilt_masks
        prebuilt_masks = {
            'no_status': cond.no_status,
            'erm_in_range': cond.in_date_range,
            'erm_le_date': cond.erm_before_or_equal_file_date,
            'erm_gt_date': cond.erm_after_file_date,
            'qty_matched': cond.quantity_matched,
            'not_billed': cond.not_billed,
            'has_billing': cond.has_billing,
            'fully_billed': cond.fully_billed,
            'has_unpaid': cond.has_unpaid_amount,
            'remark_completed': (cond.procurement_completed_or_rent
                                 | cond.fn_completed_or_posted),
            'pr_not_incomplete': cond.pr_not_incomplete,
            'is_fa': cond.is_fa,
            'not_fa': ~cond.is_fa,
            'not_error': cond.procurement_not_error,
            'out_of_range': cond.out_of_date_range,
            'format_error': cond.format_error,
        }

        engine_context = {
            'processing_date': df['æª”æ¡ˆæ—¥æœŸ'].iloc[0] if 'æª”æ¡ˆæ—¥æœŸ' in df.columns else None,
            'prebuilt_masks': prebuilt_masks,
        }

        self.logger.info("ğŸ”„ å¼•æ“é©…å‹•: åŸ·è¡Œ ERM é…ç½®åŒ–æ¢ä»¶...")
        df, stats = self.engine.apply_rules(
            df, status_column, engine_context,
            processing_type='PO' if 'POç‹€æ…‹' == status_column else 'PR',
            update_no_status=True
        )

        # è¨˜éŒ„çµ±è¨ˆ
        total_hits = sum(stats.values())
        self.logger.info(
            f"âœ… ERM å¼•æ“é©…å‹•å®Œæˆ: {len(stats)} æ¢è¦å‰‡, "
            f"å…±å‘½ä¸­ {total_hits:,} ç­†"
        )

        return df
    
    def _log_condition_result(self, condition_name: str, count: int):
        """è¨˜éŒ„æ¢ä»¶åˆ¤æ–·çµæœ"""
        if count > 0:
            self.logger.debug(f"æ¢ä»¶ [{condition_name}]: {count} ç­†ç¬¦åˆ")
    
    # ========== éšæ®µ 4: è™•ç†æ ¼å¼éŒ¯èª¤ ==========
    
    def _handle_format_errors(self, df: pd.DataFrame, 
                              cond: ERMConditions,
                              status_column: str) -> pd.DataFrame:
        """è™•ç†æ ¼å¼éŒ¯èª¤çš„è¨˜éŒ„"""
        mask_format_error = cond.no_status & cond.format_error
        df.loc[mask_format_error, status_column] = 'æ ¼å¼éŒ¯èª¤ï¼Œé€€å–®'
        
        error_count = mask_format_error.sum()
        if error_count > 0:
            self.logger.warning(f"ç™¼ç¾ {error_count} ç­†æ ¼å¼éŒ¯èª¤")
        
        return df
    
    # ========== éšæ®µ 5: è¨­ç½®æ˜¯å¦ä¼°è¨ˆå…¥å¸³ ==========
    
    def _set_accrual_flag(self, df: pd.DataFrame, status_column: str) -> pd.DataFrame:
        """
        æ ¹æ“š PO/PRç‹€æ…‹ è¨­ç½®æ˜¯å¦ä¼°è¨ˆå…¥å¸³
        
        SPX é‚è¼¯ï¼šåªæœ‰ã€Œå·²å®Œæˆã€ç‹€æ…‹éœ€è¦ä¼°åˆ—å…¥å¸³
        """
        mask_completed = df[status_column].str.contains('å·²å®Œæˆ', na=False)
        
        df.loc[mask_completed, 'æ˜¯å¦ä¼°è¨ˆå…¥å¸³'] = 'Y'
        df.loc[~mask_completed, 'æ˜¯å¦ä¼°è¨ˆå…¥å¸³'] = 'N'
        
        accrual_count = mask_completed.sum()
        self.logger.info(f"è¨­ç½®ä¼°åˆ—æ¨™è¨˜ï¼š{accrual_count} ç­†éœ€ä¼°åˆ—")
        
        return df
    
    # ========== éšæ®µ 6: è¨­ç½®æœƒè¨ˆæ¬„ä½ ==========
    
    def _set_accounting_fields(self, df: pd.DataFrame,
                               ref_account: pd.DataFrame,
                               ref_liability: pd.DataFrame) -> pd.DataFrame:
        """è¨­ç½®æ‰€æœ‰æœƒè¨ˆç›¸é—œæ¬„ä½"""
        
        need_accrual = df['æ˜¯å¦ä¼°è¨ˆå…¥å¸³'] == 'Y'
        
        # 1. Account code
        df.loc[need_accrual, 'Account code'] = df.loc[need_accrual, 'GL#']
        
        # 2. Account Nameï¼ˆé€šé mergeï¼‰
        df = self._set_account_name(df, ref_account, need_accrual)
        
        # 3. Product code
        df.loc[need_accrual, 'Product code'] = df.loc[need_accrual, 'Product Code']
        
        # 4. Region_cï¼ˆSPX å›ºå®šå€¼ï¼‰
        col_defaults = config_manager._config_toml.get('spx_column_defaults', {})
        df.loc[need_accrual, 'Region_c'] = col_defaults.get('region', 'TW')
        
        # 5. Dep.ï¼ˆéƒ¨é–€ä»£ç¢¼ï¼‰
        df = self._set_department(df, need_accrual)
        
        # 6. Currency_c
        df.loc[need_accrual, 'Currency_c'] = df.loc[need_accrual, 'Currency']
        
        # 7. Accr. Amountï¼ˆé ä¼°é‡‘é¡ï¼‰
        df = self._calculate_accrual_amount(df, need_accrual)
        
        # 8. é ä»˜æ¬¾è™•ç†
        df = self._handle_prepayment(df, need_accrual, ref_liability)
        
        self.logger.info("æœƒè¨ˆæ¬„ä½è¨­ç½®å®Œæˆ")
        
        return df
    
    def _set_account_name(self, df: pd.DataFrame, ref_account: pd.DataFrame,
                          mask: pd.Series) -> pd.DataFrame:
        """è¨­ç½®æœƒè¨ˆç§‘ç›®åç¨±"""
        if ref_account.empty:
            self.logger.warning("åƒè€ƒç§‘ç›®è³‡æ–™ç‚ºç©º")
            return df
        
        # ä½¿ç”¨ merge å¾åƒè€ƒè³‡æ–™å–å¾—ç§‘ç›®åç¨±
        merged = pd.merge(
            df, 
            ref_account[['Account', 'Account Desc']],
            how='left',
            left_on='Account code',
            right_on='Account'
        )
        
        df['Account Name'] = merged['Account Desc']
        
        return df
    
    def _set_department(self, df: pd.DataFrame, mask: pd.Series) -> pd.DataFrame:
        """
        è¨­ç½®éƒ¨é–€ä»£ç¢¼
        
        è¦å‰‡ï¼š
        - å¦‚æœç§‘ç›®åœ¨ dept_accounts æ¸…å–®ä¸­ï¼Œå– Department å‰3ç¢¼
        - å¦å‰‡è¨­ç‚º '000'
        """
        isin_dept = df['Account code'].astype('string').isin(
            [str(x) for x in self.dept_accounts]
        )
        
        # åœ¨ dept_accounts ä¸­çš„ç§‘ç›®
        df.loc[mask & isin_dept, 'Dep.'] = \
            df.loc[mask & isin_dept, 'Department'].str[:3]
        
        # ä¸åœ¨ dept_accounts ä¸­çš„ç§‘ç›®
        col_defaults = config_manager._config_toml.get('spx_column_defaults', {})
        df.loc[mask & ~isin_dept, 'Dep.'] = col_defaults.get('default_department', '000')
        
        return df
    
    def _calculate_accrual_amount(self, df: pd.DataFrame, 
                                  mask: pd.Series) -> pd.DataFrame:
        """
        è¨ˆç®—é ä¼°é‡‘é¡
        
        å…¬å¼ï¼šUnit Price Ã— (Entry Quantity - Billed Quantity)
        """
        df['temp_amount'] = (
            df['Unit Price'].astype('Float64') * 
            (df['Entry Quantity'].astype('Float64') - 
             df['Billed Quantity'].astype('Float64'))
        )
        
        df.loc[mask, 'Accr. Amount'] = df.loc[mask, 'temp_amount']
        df.drop('temp_amount', axis=1, inplace=True)
        
        return df
    
    def _handle_prepayment(self, df: pd.DataFrame, mask: pd.Series,
                           ref_liability: pd.DataFrame) -> pd.DataFrame:
        """
        è™•ç†é ä»˜æ¬¾å’Œè² å‚µç§‘ç›®
        
        è¦å‰‡ï¼š
        - æœ‰é ä»˜æ¬¾ï¼šæ˜¯å¦æœ‰é ä»˜ = 'Y'ï¼ŒLiability = '111112'
        - ç„¡é ä»˜æ¬¾ï¼šå¾åƒè€ƒè³‡æ–™æŸ¥æ‰¾ Liability
        """
        is_prepayment = df['Entry Prepay Amount'] != '0'
        df.loc[mask & is_prepayment, 'æ˜¯å¦æœ‰é ä»˜'] = 'Y'
        
        # è¨­ç½® Liabilityï¼ˆç„¡é ä»˜æ¬¾çš„æƒ…æ³ï¼‰
        if not ref_liability.empty:
            merged = pd.merge(
                df,
                ref_liability[['Account', 'Liability']],
                how='left',
                left_on='Account code',
                right_on='Account'
            )
            df['Liability'] = merged['Liability_y']
        
        # æœ‰é ä»˜æ¬¾çš„æƒ…æ³ï¼Œè¦†è“‹ç‚º '111112'
        col_defaults = config_manager._config_toml.get('spx_column_defaults', {})
        df.loc[mask & is_prepayment, 'Liability'] = col_defaults.get(
            'prepay_liability', '111112'
        )
        
        return df
    
    # ========== éšæ®µ 7: PR Product Code æª¢æŸ¥ ==========
    
    def _check_pr_product_code(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æª¢æŸ¥ PR çš„ Product Code æ˜¯å¦èˆ‡ Project ä¸€è‡´
        
        è¦å‰‡ï¼š
        å¾ Project æ¬„ä½æå–ç¬¬ä¸€å€‹è©ï¼Œèˆ‡ Product code æ¯”å°
        - ä¸€è‡´ï¼šgood
        - ä¸ä¸€è‡´ï¼šbad
        """
        if 'Product code' not in df.columns or 'Project' not in df.columns:
            self.logger.warning("ç¼ºå°‘ Product code æˆ– Project æ¬„ä½ï¼Œè·³éæª¢æŸ¥")
            return df
        
        mask = df['Product code'].notnull()
        
        try:
            # æå– Project çš„ç¬¬ä¸€å€‹è©
            project_first_word = df.loc[mask, 'Project'].str.findall(
                r'^(\w+(?:))'
            ).apply(lambda x: x[0] if len(x) > 0 else '')
            
            # æ¯”å°
            product_match = (project_first_word == df.loc[mask, 'Product code'])
            
            df.loc[mask, 'PR Product Code Check'] = np.where(
                product_match, 'good', 'bad'
            )
            
            bad_count = (~product_match).sum()
            if bad_count > 0:
                self.logger.warning(f"ç™¼ç¾ {bad_count} ç­† PR Product Code ä¸ä¸€è‡´")
                
        except Exception as e:
            self.logger.error(f"PR Product Code æª¢æŸ¥å¤±æ•—: {str(e)}")
        
        return df
    
    # ========== è¼”åŠ©æ–¹æ³• ==========
    
    def _generate_statistics(self, df: pd.DataFrame, status_column: str) -> Dict[str, Any]:
        """ç”Ÿæˆçµ±è¨ˆè³‡è¨Š"""
        stats = {
            'total_count': len(df),
            'accrual_count': (df['æ˜¯å¦ä¼°è¨ˆå…¥å¸³'] == 'Y').sum(),
            'status_distribution': {}
        }
        
        if status_column in df.columns:
            status_counts = df[status_column].value_counts().to_dict()
            stats['status_distribution'] = {
                str(k): int(v) for k, v in status_counts.items()
            }
        
        return stats
    
    # ========== é©—è­‰æ–¹æ³• ==========
    
    async def validate_input(self, context: ProcessingContext) -> bool:
        """é©—è­‰è¼¸å…¥æ•¸æ“š"""
        df = context.data
        
        if df is None or df.empty:
            self.logger.error("è¼¸å…¥æ•¸æ“šç‚ºç©º")
            context.add_error("è¼¸å…¥æ•¸æ“šç‚ºç©º")
            return False
        
        # æª¢æŸ¥å¿…è¦æ¬„ä½
        required_columns = [
            'GL#', 'Expected Received Month_è½‰æ›æ ¼å¼',
            'YMs of Item Description', 'Entry Quantity',
            'Received Quantity', 'Billed Quantity',
            'Entry Amount', 'Entry Billed Amount',
            'Item Description', 'Remarked by Procurement',
            'Remarked by ä¸Šæœˆ FN', 'Unit Price', 'Currency',
            'Product Code'
        ]
        
        missing = [col for col in required_columns if col not in df.columns]
        
        if missing:
            self.logger.error(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {missing}")
            context.add_error(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {missing}")
            return False
        
        # æª¢æŸ¥åƒè€ƒæ•¸æ“š
        ref_account = context.get_auxiliary_data('reference_account')
        ref_liability = context.get_auxiliary_data('reference_liability')
        
        if ref_account is None or ref_liability is None:
            self.logger.error("ç¼ºå°‘åƒè€ƒæ•¸æ“š")
            context.add_error("ç¼ºå°‘åƒè€ƒæ•¸æ“š")
            return False
        
        # æª¢æŸ¥è™•ç†æ—¥æœŸ
        processing_date = context.get_variable('processing_date')
        if processing_date is None:
            self.logger.error("ç¼ºå°‘è™•ç†æ—¥æœŸ")
            context.add_error("ç¼ºå°‘è™•ç†æ—¥æœŸ")
            return False
        
        self.logger.info("è¼¸å…¥é©—è­‰é€šé")
        return True
    
    async def rollback(self, context: ProcessingContext, error: Exception):
        """å›æ»¾æ“ä½œï¼ˆå¦‚éœ€è¦ï¼‰"""
        self.logger.warning(f"å›æ»¾ ERM é‚è¼¯ï¼š{str(error)}")
        # SPX ERM æ­¥é©Ÿé€šå¸¸ä¸éœ€è¦ç‰¹æ®Šå›æ»¾æ“ä½œ


class PPEContractDateUpdateStep(PipelineStep):
    """
    PPE åˆç´„æ—¥æœŸæ›´æ–°æ­¥é©Ÿ
    
    åŠŸèƒ½ï¼š
    çµ±ä¸€åŒä¸€åº—è™Ÿï¼ˆsp_codeï¼‰çš„åˆç´„èµ·æ­¢æ—¥æœŸ
    """
    
    def __init__(self, name: str = "PPEContractDateUpdate", **kwargs):
        super().__init__(name, description="Update contract dates", **kwargs)
    
    async def execute(self, context: ProcessingContext) -> StepResult:
        """åŸ·è¡Œæ—¥æœŸæ›´æ–°"""
        start_time = datetime.now()
        
        try:
            df = context.data.copy()
            
            # æ›´æ–°åˆç´„æ—¥æœŸ
            df_updated = self._update_contract_dates(df)
            
            context.update_data(df_updated)
            
            duration = (datetime.now() - start_time).total_seconds()
            
            return StepResult(
                step_name=self.name,
                status=StepStatus.SUCCESS,
                data=df_updated,
                message="åˆç´„æ—¥æœŸæ›´æ–°å®Œæˆ",
                duration=duration
            )
            
        except Exception as e:
            self.logger.error(f"æ—¥æœŸæ›´æ–°å¤±æ•—: {str(e)}", exc_info=True)
            return StepResult(
                step_name=self.name,
                status=StepStatus.FAILED,
                error=e,
                message=str(e)
            )
    
    def _update_contract_dates(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ›´æ–°åˆç´„æ—¥æœŸï¼ˆè¤‡è£½è‡ª SpxPpeProcessorï¼‰"""
        df_updated = df.copy()
        
        # è½‰æ›æ—¥æœŸæ ¼å¼
        date_columns = [
            'contract_start_day_filing', 
            'contract_end_day_filing',
            'contract_start_day_renewal', 
            'contract_end_day_renewal'
        ]
        
        for col in date_columns:
            if col in df_updated.columns:
                df_updated[col] = pd.to_datetime(df_updated[col], errors='coerce')
        
        # æŒ‰ sp_code åˆ†çµ„æ›´æ–°
        for sp_code in df_updated['sp_code'].unique():
            mask = df_updated['sp_code'] == sp_code
            sp_data = df_updated[mask]
            
            # æ”¶é›†æ‰€æœ‰æ—¥æœŸ
            start_dates = []
            end_dates = []
            
            for col in ['contract_start_day_filing', 'contract_start_day_renewal']:
                if col in df_updated.columns:
                    dates = sp_data[col].dropna().tolist()
                    start_dates.extend(dates)
            
            for col in ['contract_end_day_filing', 'contract_end_day_renewal']:
                if col in df_updated.columns:
                    dates = sp_data[col].dropna().tolist()
                    end_dates.extend(dates)
            
            # æ›´æ–°ç‚ºæœ€å°èµ·å§‹æ—¥å’Œæœ€å¤§çµæŸæ—¥
            if start_dates:
                min_start = min(start_dates)
                for col in ['contract_start_day_filing', 'contract_start_day_renewal']:
                    if col in df_updated.columns:
                        df_updated.loc[mask, col] = min_start
            
            if end_dates:
                max_end = max(end_dates)
                for col in ['contract_end_day_filing', 'contract_end_day_renewal']:
                    if col in df_updated.columns:
                        df_updated.loc[mask, col] = max_end
        
        return df_updated.drop_duplicates()

    async def validate_input(self, context: ProcessingContext) -> bool:
        """é©—è­‰è¼¸å…¥"""
        if context.data is None or context.data.empty:
            self.logger.error("No data for update")
            return False
        
        return True

class PPEMonthDifferenceStep(PipelineStep):
    """
    PPE æœˆä»½å·®ç•°è¨ˆç®—æ­¥é©Ÿ
    
    åŠŸèƒ½ï¼š
    è¨ˆç®—åˆç´„çµæŸæ—¥æœŸèˆ‡ç•¶å‰æœˆä»½çš„å·®ç•°
    """
    
    def __init__(self, 
                 name: str = "PPEMonthDifference",
                 current_month: int = None,
                 **kwargs):
        super().__init__(name, description="Calculate month difference", **kwargs)
        self.current_month = current_month
    
    async def execute(self, context: ProcessingContext) -> StepResult:
        """åŸ·è¡Œæœˆä»½å·®ç•°è¨ˆç®—"""
        start_time = datetime.now()
        
        try:
            df = context.data.copy()
            
            # ç²å–ç•¶å‰æœˆä»½
            current_month = (self.current_month or 
                             context.get_variable('current_month'))
            
            if not current_month:
                raise ValueError("æœªæä¾›ç•¶å‰æœˆä»½åƒæ•¸")
            
            # é¸æ“‡å¿…è¦æ¬„ä½
            selected_cols = [
                'sp_code', 
                'address', 
                'contract_start_day_filing', 
                'contract_end_day_renewal'
            ]
            
            # è¨ˆç®—æœˆä»½å·®ç•°
            df_result = self._calculate_month_difference(
                df[selected_cols],
                'contract_end_day_renewal',
                current_month
            )
            
            # æ–°å¢æˆªæ–·åœ°å€æ¬„ä½ï¼ˆç”¨æ–¼åœ°å€æ¨¡ç³ŠåŒ¹é…ï¼‰
            df_result['truncated_address'] = df_result['address'].apply(
                self._truncate_address_at_hao
            )
            
            context.update_data(df_result)
            
            duration = (datetime.now() - start_time).total_seconds()
            
            metadata = (StepMetadataBuilder()
                        .set_row_counts(len(df), len(df_result))
                        .set_time_info(start_time, datetime.now())
                        .add_custom('current_month', current_month)
                        .add_custom('average_months_diff', 
                                    float(df_result['months_diff'].mean()))
                        .build())
            
            return StepResult(
                step_name=self.name,
                status=StepStatus.SUCCESS,
                data=df_result,
                message=f"æœˆä»½å·®ç•°è¨ˆç®—å®Œæˆ: ç•¶å‰æœˆä»½ {current_month}",
                duration=duration,
                metadata=metadata
            )
            
        except Exception as e:
            self.logger.error(f"æœˆä»½å·®ç•°è¨ˆç®—å¤±æ•—: {str(e)}", exc_info=True)
            return StepResult(
                step_name=self.name,
                status=StepStatus.FAILED,
                error=e,
                message=str(e)
            )
    
    def _calculate_month_difference(self, df: pd.DataFrame, 
                                    date_column: str, 
                                    target_ym: int) -> pd.DataFrame:
        """è¨ˆç®—æœˆä»½å·®ç•°"""
        df_result = df.copy()
        
        # ç¢ºä¿æ—¥æœŸæ ¼å¼
        df_result[date_column] = pd.to_datetime(df_result[date_column])
        
        # ç›®æ¨™æ—¥æœŸ
        target_year = target_ym // 100
        target_month = target_ym % 100
        target_date = datetime(target_year, target_month, 1)
        
        # è¨ˆç®—å·®ç•°
        def months_difference(date1, date2):
            return (date1.year - date2.year) * 12 + (date1.month - date2.month)
        
        df_result['months_diff'] = df_result[date_column].apply(
            lambda x: months_difference(x, target_date)
        ).add(1)
        
        return df_result
    
    def _truncate_address_at_hao(self, address: str) -> str:
        """æˆªæ–·åœ°å€åˆ°ã€Œè™Ÿã€"""
        if not isinstance(address, str):
            return address
        
        pattern = r'^.*?è™Ÿ'
        match = re.search(pattern, address)
        return match.group(0) if match else address

    async def validate_input(self, context: ProcessingContext) -> bool:
        """é©—è­‰è¼¸å…¥"""
        if context.data is None or context.data.empty:
            self.logger.error("No data for calculating difference")
            return False
        
        return True