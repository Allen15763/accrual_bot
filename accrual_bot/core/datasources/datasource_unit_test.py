"""
æ•¸æ“šæºæ¨¡çµ„æ¸¬è©¦è…³æœ¬
ç”¨æ–¼æ¸¬è©¦å„ç¨®æ•¸æ“šæºçš„åŠŸèƒ½
"""

import asyncio
import pandas as pd
import numpy as np
from pathlib import Path
import logging
import sys
import time
import gc

# æ·»åŠ å°ˆæ¡ˆè·¯å¾‘
current_dir = Path(__file__).parent.parent.parent
sys.path.insert(0, str(current_dir))

from core.datasources import (
    DataSourceFactory, DataSourceConfig, DataSourceType,
    ExcelSource, CSVSource, ParquetSource, DuckDBSource
)

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("DataSourceTest")


async def test_excel_source():
    """æ¸¬è©¦Excelæ•¸æ“šæº"""
    logger.info("=== æ¸¬è©¦Excelæ•¸æ“šæº ===")
    
    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    test_data = pd.DataFrame({
        'PO#': ['PO001', 'PO002', 'PO003'],
        'Amount': [1000, 2000, 3000],
        'Date': pd.date_range('2025-01-01', periods=3)
    })
    
    # æ¸¬è©¦æ–‡ä»¶è·¯å¾‘
    test_file = Path('test_data.xlsx')
    
    try:
        # å‰µå»ºExcelæ•¸æ“šæº
        config = DataSourceConfig(
            source_type=DataSourceType.EXCEL,
            connection_params={'file_path': str(test_file)}
        )
        
        # å…ˆå¯«å…¥æ¸¬è©¦æ•¸æ“š
        test_data.to_excel(test_file, index=False)
        
        source = DataSourceFactory.create(config)
        
        # æ¸¬è©¦è®€å–
        df = await source.read()
        logger.info(f"è®€å–åˆ° {len(df)} è¡Œæ•¸æ“š")
        logger.info(f"åˆ—: {df.columns.tolist()}")
        
        # æ¸¬è©¦å¯«å…¥
        new_data = pd.DataFrame({
            'PO#': ['PO004'],
            'Amount': [4000],
            'Date': [pd.Timestamp('2025-01-04')]
        })
        
        success = await source.write(new_data, sheet_name='NewSheet')
        logger.info(f"å¯«å…¥æ–°å·¥ä½œè¡¨: {'æˆåŠŸ' if success else 'å¤±æ•—'}")
        
        # ç²å–å…ƒæ•¸æ“š
        metadata = source.get_metadata()
        logger.info(f"Excelå…ƒæ•¸æ“š: {metadata}")
        
        # æ¸…ç†
        await source.close()
        # æ–°çš„ç·šç¨‹å†è¦æ“ä½œæ™‚ï¼ŒèˆŠçš„ç·šç¨‹å»è¦ç§»é™¤Path('test_data.xlsx')ï¼Œå› ç‚ºExcelSourceä½¿ç”¨pd.ExcelFileæ²’æœ‰ç”¨withæ­£ç¢ºé—œé–‰ï¼Œæœƒå°è‡´éŒ¯èª¤ã€‚
        # ERROR - Excelæ•¸æ“šæºç•°å¸¸: [WinError 32] ç¨‹åºç„¡æ³•å­˜å–æª”æ¡ˆï¼Œå› ç‚ºæª”æ¡ˆæ­£ç”±å¦ä¸€å€‹ç¨‹åºä½¿ç”¨ã€‚: 'test_data.xlsx'
        test_file.unlink(missing_ok=True)
        
        return True
        
    except Exception as e:
        logger.error(f"Excelæ¸¬è©¦å¤±æ•—: {str(e)}")
        test_file.unlink(missing_ok=True)
        return False


async def test_csv_source():
    """æ¸¬è©¦CSVæ•¸æ“šæº"""
    logger.info("=== æ¸¬è©¦CSVæ•¸æ“šæº ===")
    
    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    test_data = pd.DataFrame({
        'PR#': ['PR001', 'PR002', 'PR003'],
        'Description': ['Item 1', 'Item 2', 'Item 3'],
        'Quantity': [10, 20, 30]
    })
    
    # æ¸¬è©¦æ–‡ä»¶è·¯å¾‘
    test_file = Path('test_data.csv')
    
    try:
        # å…ˆå‰µå»ºæ¸¬è©¦æ–‡ä»¶
        test_data.to_csv(test_file, index=False)
        
        # å‰µå»ºCSVæ•¸æ“šæº
        source = CSVSource.create_from_file(str(test_file))
        
        # æ¸¬è©¦è®€å–
        df = await source.read()
        logger.info(f"CSVè®€å–: {len(df)} è¡Œ")
        
        # æ¸¬è©¦æŸ¥è©¢
        filtered = await source.read(query="Quantity > 15")
        logger.info(f"ç¯©é¸å¾Œ: {len(filtered)} è¡Œ")
        
        # æ¸¬è©¦è¿½åŠ 
        new_data = pd.DataFrame({
            'PR#': ['PR004'],
            'Description': ['Item 4'],
            'Quantity': [40]
        })
        
        success = await source.append_data(new_data)
        logger.info(f"è¿½åŠ æ•¸æ“š: {'æˆåŠŸ' if success else 'å¤±æ•—'}")
        
        # é©—è­‰è¿½åŠ çµæœ
        df_after = await source.read()
        logger.info(f"è¿½åŠ å¾Œç¸½è¡Œæ•¸: {len(df_after)}")
        
        # æ¸…ç†
        await source.close()
        test_file.unlink(missing_ok=True)
        
        return True
        
    except Exception as e:
        logger.error(f"CSVæ¸¬è©¦å¤±æ•—: {str(e)}")
        test_file.unlink(missing_ok=True)
        return False


async def test_parquet_source():
    """æ¸¬è©¦Parquetæ•¸æ“šæº"""
    logger.info("=== æ¸¬è©¦Parquetæ•¸æ“šæº ===")
    
    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    test_data = pd.DataFrame({
        'ID': range(1000),
        'Value': np.random.randn(1000),
        'Category': np.random.choice(['A', 'B', 'C'], 1000)
    })
    
    # æ¸¬è©¦æ–‡ä»¶è·¯å¾‘
    test_file = Path('test_data.parquet')
    
    try:
        # å‰µå»ºParquetæ•¸æ“šæº
        source = ParquetSource.create_from_file(str(test_file))
        
        # æ¸¬è©¦å¯«å…¥
        success = await source.write(test_data)
        logger.info(f"Parquetå¯«å…¥: {'æˆåŠŸ' if success else 'å¤±æ•—'}")
        
        # æ¸¬è©¦è®€å–
        df = await source.read()
        logger.info(f"Parquetè®€å–: {len(df)} è¡Œ")
        
        # æ¸¬è©¦åˆ—ç¯©é¸
        df_subset = await source.read(columns=['ID', 'Value'])
        logger.info(f"åˆ—ç¯©é¸å¾Œ: {df_subset.columns.tolist()}")
        
        # ç²å–å…ƒæ•¸æ“š
        metadata = source.get_metadata()
        logger.info(f"Parquetå…ƒæ•¸æ“š: è¡Œæ•¸={metadata.get('num_rows')}, "
                    f"åˆ—æ•¸={metadata.get('num_columns')}")
        
        # ç²å–schema
        schema = await source.get_schema()
        logger.info(f"Schema: {schema}")
        
        # æ¸…ç†
        await source.close()
        test_file.unlink(missing_ok=True)
        
        return True
        
    except Exception as e:
        logger.error(f"Parquetæ¸¬è©¦å¤±æ•—: {str(e)}")
        test_file.unlink(missing_ok=True)
        return False


async def test_duckdb_source():
    """æ¸¬è©¦DuckDBæ•¸æ“šæº"""
    logger.info("=== æ¸¬è©¦DuckDBæ•¸æ“šæº ===")
    
    # å‰µå»ºæ¸¬è©¦æ•¸æ“š
    test_data = pd.DataFrame({
        'order_id': range(1, 101),
        'customer': [f'Customer_{i%10}' for i in range(100)],
        'amount': np.random.uniform(100, 1000, 100),
        'date': pd.date_range('2025-01-01', periods=100)
    })
    
    try:
        # å‰µå»ºå…§å­˜æ•¸æ“šåº«
        source = DuckDBSource.create_memory_db()
        
        # å¯«å…¥æ•¸æ“š
        success = await source.write(test_data, table_name='orders')
        logger.info(f"DuckDBå¯«å…¥: {'æˆåŠŸ' if success else 'å¤±æ•—'}")
        
        # SQLæŸ¥è©¢
        result = await source.read("SELECT * FROM orders WHERE amount > 500")
        logger.info(f"æŸ¥è©¢çµæœ: {len(result)} è¡Œ")
        
        # èšåˆæŸ¥è©¢
        agg_result = await source.read("""
            SELECT customer, 
                   COUNT(*) as order_count,
                   AVG(amount) as avg_amount
            FROM orders 
            GROUP BY customer
        """)
        logger.info(f"èšåˆçµæœ: {len(agg_result)} å€‹å®¢æˆ¶")
        
        # å‰µå»ºæ–°è¡¨
        await source.create_table('summary', {
            'customer': 'VARCHAR',
            'total_amount': 'DOUBLE'
        })
        
        # æ’å…¥èšåˆæ•¸æ“š
        await source.execute("""
            INSERT INTO summary
            SELECT customer, SUM(amount) as total_amount
            FROM orders
            GROUP BY customer
        """)
        
        # é©—è­‰
        summary = await source.read("SELECT * FROM summary")
        logger.info(f"Summaryè¡¨: {len(summary)} è¡Œ")
        
        # åˆ—å‡ºæ‰€æœ‰è¡¨
        tables = await source.list_tables()
        logger.info(f"æ‰€æœ‰è¡¨: {tables}")
        
        # ç²å–è¡¨ä¿¡æ¯
        table_info = await source.get_table_info('orders')
        logger.info(f"Ordersè¡¨ä¿¡æ¯: è¡Œæ•¸={table_info.get('row_count')}")
        
        # é—œé–‰é€£æ¥
        await source.close()
        
        return True
        
    except Exception as e:
        logger.error(f"DuckDBæ¸¬è©¦å¤±æ•—: {str(e)}")
        return False


async def safe_file_cleanup_async(file_path: Path, max_retries: int = 5, 
                                  initial_delay: float = 0.5) -> bool:
    """
    ç•°æ­¥ç‰ˆæœ¬çš„å®‰å…¨æª”æ¡ˆæ¸…ç†ï¼Œè™•ç†Windowsæ–‡ä»¶é–å®šå•é¡Œ
    
    Args:
        file_path: è¦åˆªé™¤çš„æª”æ¡ˆè·¯å¾‘
        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
        initial_delay: åˆå§‹é‡è©¦é–“éš”ï¼ˆç§’ï¼‰
        
    Returns:
        bool: æ˜¯å¦æˆåŠŸåˆªé™¤
    """
    if not file_path.exists():
        return True
        
    for attempt in range(max_retries):
        try:
            # å¼·åˆ¶åƒåœ¾å›æ”¶ï¼Œå¹«åŠ©é‡‹æ”¾è³‡æº
            gc.collect()
            
            # å˜—è©¦åˆªé™¤æª”æ¡ˆ
            file_path.unlink()
            return True
            
        except (PermissionError, OSError) as e:
            if attempt < max_retries - 1:
                # è¨ˆç®—å»¶é²æ™‚é–“ï¼ˆæŒ‡æ•¸é€€é¿ï¼‰
                delay = initial_delay * (2 ** attempt)
                logging.getLogger(__name__).warning(
                    f"åˆªé™¤æª”æ¡ˆå¤±æ•— (å˜—è©¦ {attempt + 1}/{max_retries}): {e}"
                    f"ï¼Œç­‰å¾… {delay} ç§’å¾Œé‡è©¦..."
                )
                await asyncio.sleep(delay)
                
                # æ¯æ¬¡é‡è©¦å‰éƒ½é€²è¡Œåƒåœ¾å›æ”¶
                gc.collect()
            else:
                # æœ€å¾Œä¸€æ¬¡å˜—è©¦å¤±æ•—
                logging.getLogger(__name__).error(
                    f"ç„¡æ³•åˆªé™¤æª”æ¡ˆ {file_path} (å·²é‡è©¦ {max_retries} æ¬¡): {e}"
                )
                return False
                
    return False

async def test_data_migration():
    """æ¸¬è©¦æ•¸æ“šé·ç§»ï¼ˆå¾Excelåˆ°DuckDBï¼‰- ä¿®å¾©ç‰ˆæœ¬"""
    logger.info("=== æ¸¬è©¦æ•¸æ“šé·ç§» ===")
    
    # æº–å‚™æ¸¬è©¦æ•¸æ“š
    test_data = pd.DataFrame({
        'PO#': [f'PO{i:04d}' for i in range(1, 21)],
        'Supplier': [f'Supplier_{i%5}' for i in range(20)],
        'Amount': np.random.uniform(1000, 10000, 20),
        'Status': np.random.choice(['Pending', 'Approved', 'Completed'], 20)
    })
    
    excel_file = Path('migration_test.xlsx')
    db_file = Path('migration_test.db')
    
    # ç¢ºä¿é–‹å§‹å‰æª”æ¡ˆå·²æ¸…ç†
    await safe_file_cleanup_async(excel_file)
    await safe_file_cleanup_async(db_file)
    
    excel_source = None
    db_source = None
    
    try:
        # 1. ä¿å­˜åˆ°Excel
        test_data.to_excel(excel_file, index=False)
        logger.info("å‰µå»ºæºExcelæ–‡ä»¶")
        
        # 2. å¾Excelè®€å–
        excel_source = ExcelSource.create_from_file(str(excel_file))
        df = await excel_source.read()
        logger.info(f"å¾Excelè®€å–: {len(df)} è¡Œ")
        
        # 3. å¯«å…¥DuckDB
        db_source = DuckDBSource.create_file_db(str(db_file))
        success = await db_source.write(df, table_name='purchase_orders')
        logger.info(f"å¯«å…¥DuckDB: {'æˆåŠŸ' if success else 'å¤±æ•—'}")
        
        # 4. é©—è­‰é·ç§»çµæœ
        migrated_data = await db_source.read("SELECT * FROM purchase_orders")
        logger.info(f"é·ç§»é©—è­‰: åŸå§‹={len(test_data)}è¡Œ, é·ç§»å¾Œ={len(migrated_data)}è¡Œ")
        
        # 5. æ¸¬è©¦æŸ¥è©¢æ€§èƒ½
        # ExcelæŸ¥è©¢ï¼ˆéœ€è¦è®€å–å…¨éƒ¨æ•¸æ“šï¼‰
        start = time.time()
        excel_df = await excel_source.read()
        excel_filtered = excel_df[excel_df['Amount'] > 5000]
        excel_time = time.time() - start
        
        # DuckDBæŸ¥è©¢ï¼ˆç›´æ¥SQLç¯©é¸ï¼‰
        start = time.time()
        db_filtered = await db_source.read("SELECT * FROM purchase_orders WHERE Amount > 5000")
        db_time = time.time() - start
        
        logger.info(f"æŸ¥è©¢æ€§èƒ½: Excel={excel_time:.4f}ç§’, DuckDB={db_time:.4f}ç§’")
        
        # æˆåŠŸæ¨™è¨˜
        migration_success = True
        
    except Exception as e:
        logger.error(f"æ•¸æ“šé·ç§»æ¸¬è©¦å¤±æ•—: {str(e)}")
        migration_success = False
    
    finally:
        # ç¢ºä¿è³‡æºå®Œå…¨é‡‹æ”¾
        logger.info("é–‹å§‹æ¸…ç†è³‡æº...")
        
        try:
            # é—œé–‰ Excel æº
            if excel_source:
                await excel_source.close()
                excel_source = None
                logger.info("Excel æºå·²é—œé–‰")
                
            # é—œé–‰ DuckDB æº - é€™æ˜¯é—œéµæ­¥é©Ÿ
            if db_source:
                await db_source.close()
                db_source = None
                logger.info("DuckDB æºå·²é—œé–‰")
                
            # é¡å¤–ç­‰å¾…ï¼Œè®“ç³»çµ±å®Œå…¨é‡‹æ”¾è³‡æº
            logger.info("ç­‰å¾…ç³»çµ±å®Œå…¨é‡‹æ”¾æª”æ¡ˆé–å®š...")
            await asyncio.sleep(2.0)
            
            # å¼·åˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()
            await asyncio.sleep(0.5)
            
            # ä½¿ç”¨å®‰å…¨æ¸…ç†å‡½æ•¸åˆªé™¤æª”æ¡ˆ
            logger.info("é–‹å§‹æ¸…ç†æ¸¬è©¦æª”æ¡ˆ...")
            
            excel_cleanup = await safe_file_cleanup_async(excel_file)
            logger.info(f"Excel æª”æ¡ˆæ¸…ç†: {'æˆåŠŸ' if excel_cleanup else 'å¤±æ•—'}")
            
            db_cleanup = await safe_file_cleanup_async(db_file)
            logger.info(f"DuckDB æª”æ¡ˆæ¸…ç†: {'æˆåŠŸ' if db_cleanup else 'å¤±æ•—'}")
            
            if not db_cleanup:
                logger.warning(f"DuckDB æª”æ¡ˆ {db_file} ç„¡æ³•åˆªé™¤ï¼Œå¯èƒ½è¢«ç³»çµ±é–å®š")
                # åœ¨æŸäº›æƒ…æ³ä¸‹ï¼Œæˆ‘å€‘å¯ä»¥é¸æ“‡é‡å‘½åæª”æ¡ˆè€Œä¸æ˜¯åˆªé™¤
                try:
                    backup_name = db_file.with_suffix('.db.bak')
                    if backup_name.exists():
                        backup_name.unlink()
                    db_file.rename(backup_name)
                    logger.info(f"å·²å°‡ {db_file} é‡å‘½åç‚º {backup_name}")
                except Exception as rename_e:
                    logger.warning(f"é‡å‘½åä¹Ÿå¤±æ•—: {rename_e}")
            
        except Exception as cleanup_e:
            logger.error(f"æ¸…ç†éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {cleanup_e}")
    
    return migration_success

async def test_concurrent_operations():
    """æ¸¬è©¦æ”¹é€²çš„ä½µç™¼æ“ä½œï¼ˆé¿å…æ­»é–ï¼‰"""
    logger.info("=== æ¸¬è©¦ä½µç™¼æ“ä½œï¼ˆæ”¹é€²ç‰ˆï¼‰===")
    
    db_source = None
    
    try:
        # å‰µå»ºDuckDBå…§å­˜æ•¸æ“šåº« 
        """RAM DBåœ¨ä¸åŒç·šç¨‹æ˜¯ç¨ç«‹çš„ï¼Œä¸èƒ½ç”¨memoryåšä½µç™¼æ¸¬è©¦"""
        # db_source = DuckDBSource.create_memory_db()
        # å‰µå»ºæ–‡ä»¶æ•¸æ“šåº«ï¼ˆæ”¯æŒä½µç™¼è¨ªå•ï¼‰
        db_file = Path('concurrent_test.db')
        db_source = DuckDBSource.create_file_db(str(db_file))
        
        # å‰µå»ºæ¸¬è©¦æ•¸æ“š
        test_data = pd.DataFrame({
            'id': range(100),
            'value': np.random.randn(100)
        })
        
        # å…ˆå‰µå»ºæ‰€æœ‰è¡¨ï¼ˆé¿å…ä½µç™¼å‰µå»ºè¡¨çš„å•é¡Œï¼‰
        for i in range(5):
            table_data = test_data.copy()
            table_data['batch'] = i
            await db_source.write(table_data, table_name=f'table_{i}')
            logger.info(f"å‰µå»ºè¡¨ table_{i}")
        
        # ä½µç™¼è®€å–ï¼ˆé€™æ˜¯å®‰å…¨çš„ï¼‰
        read_tasks = []
        for i in range(5):
            task = db_source.read(f"SELECT COUNT(*) as cnt FROM table_{i}")
            read_tasks.append(task)
        
        # ä½¿ç”¨gatheråŸ·è¡Œä½µç™¼è®€å–
        results = await asyncio.gather(*read_tasks, return_exceptions=True)
        
        # æª¢æŸ¥çµæœ
        success_count = 0
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Table_{i} è®€å–å¤±æ•—: {result}")
            else:
                count = result['cnt'].iloc[0] if not result.empty else 0
                logger.info(f"Table_{i}: {count} è¡Œ")
                success_count += 1
        
        logger.info(f"ä½µç™¼è®€å–æˆåŠŸç‡: {success_count}/5")
        
        # æ¸¬è©¦ä½µç™¼æŸ¥è©¢
        query_tasks = []
        for i in range(3):
            query = f"""
                SELECT 
                    batch,
                    COUNT(*) as count,
                    AVG(value) as avg_value
                FROM table_{i}
                GROUP BY batch
            """
            query_tasks.append(db_source.read(query))
        
        # åŸ·è¡Œä½µç™¼æŸ¥è©¢
        query_results = await asyncio.gather(*query_tasks, return_exceptions=True)
        
        query_success = sum(1 for r in query_results if not isinstance(r, Exception))
        logger.info(f"ä½µç™¼æŸ¥è©¢æˆåŠŸ: {query_success}/3")
        
        # é—œé–‰é€£æ¥
        await db_source.close()
        await safe_file_cleanup_async(db_file)
        
        return success_count == 5 and query_success == 3
        
    except Exception as e:
        logger.error(f"ä½µç™¼æ¸¬è©¦å¤±æ•—: {str(e)}")
        if db_source:
            await db_source.close()
        await safe_file_cleanup_async(db_file)
        return False


async def test_thread_safety():
    """æ¸¬è©¦ç·šç¨‹å®‰å…¨æ€§"""
    logger.info("=== æ¸¬è©¦ç·šç¨‹å®‰å…¨æ€§ ===")
    
    db_path = Path('thread_test.db')
    
    try:
        # å‰µå»ºæ–‡ä»¶æ•¸æ“šåº«ï¼ˆæ¸¬è©¦å¤šç·šç¨‹è¨ªå•åŒä¸€æ–‡ä»¶ï¼‰
        source = DuckDBSource.create_file_db(str(db_path))
        
        # å‰µå»ºæ¸¬è©¦è¡¨
        test_data = pd.DataFrame({
            'id': range(100),
            'value': np.random.randn(100)
        })
        await source.write(test_data, table_name='test_table')
        
        # ä½µç™¼åŸ·è¡Œå¤šå€‹æ“ä½œ
        tasks = []
        
        # æ··åˆè®€å¯«æ“ä½œ
        for i in range(10):
            if i % 2 == 0:
                # è®€æ“ä½œ
                task = source.read("SELECT COUNT(*) FROM test_table")
            else:
                # å¯«æ“ä½œï¼ˆè¿½åŠ æ•¸æ“šï¼‰
                new_data = pd.DataFrame({
                    'id': [100 + i],
                    'value': [np.random.randn()]
                })
                task = source.write(new_data, table_name=f'test_{i}')
            tasks.append(task)
        
        # åŸ·è¡Œæ‰€æœ‰ä»»å‹™
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # çµ±è¨ˆæˆåŠŸçš„æ“ä½œ
        success_count = sum(1 for r in results if not isinstance(r, Exception))
        error_count = sum(1 for r in results if isinstance(r, Exception))
        
        logger.info(f"ç·šç¨‹å®‰å…¨æ¸¬è©¦: æˆåŠŸ={success_count}, å¤±æ•—={error_count}")
        
        # æ¸…ç†
        await source.close()
        # db_path.unlink(missing_ok=True)
        await safe_file_cleanup_async(db_path)
        
        return error_count == 0
        
    except Exception as e:
        logger.error(f"ç·šç¨‹å®‰å…¨æ¸¬è©¦å¤±æ•—: {str(e)}")
        # db_path.unlink(missing_ok=True)
        await safe_file_cleanup_async(db_path)
        return False


async def test_deadlock_prevention():
    """æ¸¬è©¦æ­»é–é é˜²æ©Ÿåˆ¶"""
    logger.info("=== æ¸¬è©¦æ­»é–é é˜² ===")
    
    try:
        # æ¸¬è©¦1: DuckDBå¤šç·šç¨‹è¨ªå•
        logger.info("æ¸¬è©¦1: DuckDBå¤šç·šç¨‹è¨ªå•")
        db_path = 'deadlock_test.duckdb'
        
        # db_source = DuckDBSource.create_memory_db()
        db_source = DuckDBSource.create_file_db(db_path)
        
        # å‰µå»ºåŸºç¤æ•¸æ“š
        base_data = pd.DataFrame({
            'id': range(1000),
            'value': np.random.randn(1000)
        })
        await db_source.write(base_data, table_name='base_table')
        
        # é«˜ä½µç™¼è®€å¯«æ¸¬è©¦
        tasks = []
        for i in range(20):  # 20å€‹ä½µç™¼æ“ä½œ
            if i % 3 == 0:
                # è¤‡é›œæŸ¥è©¢
                task = db_source.read("""
                    SELECT 
                        id % 10 as group_id,
                        COUNT(*) as count,
                        AVG(value) as avg_value,
                        MIN(value) as min_value,
                        MAX(value) as max_value
                    FROM base_table
                    GROUP BY id % 10
                    ORDER BY group_id
                """)
            elif i % 3 == 1:
                # ç°¡å–®æŸ¥è©¢
                task = db_source.read(f"SELECT * FROM base_table WHERE id = {i}")
            else:
                # å¯«å…¥æ–°è¡¨
                new_data = pd.DataFrame({
                    'id': [i],
                    'value': [np.random.randn()]
                })
                task = db_source.write(new_data, table_name=f'concurrent_table_{i}')
            tasks.append(task)
        
        # åŸ·è¡Œä½µç™¼ä»»å‹™
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # æª¢æŸ¥çµæœ
        exceptions = [r for r in results if isinstance(r, Exception)]
        if exceptions:
            for e in exceptions[:3]:  # é¡¯ç¤ºå‰3å€‹éŒ¯èª¤
                logger.error(f"ä½µç™¼éŒ¯èª¤: {e}")
        
        test1_success = len(exceptions) == 0
        logger.info(f"DuckDBä½µç™¼æ¸¬è©¦: {'âœ… é€šé' if test1_success else 'âŒ å¤±æ•—'} ({len(exceptions)} éŒ¯èª¤)")
        
        await db_source.close()
        Path(db_path).unlink()
        
        # æ¸¬è©¦2: å¤šæ•¸æ“šæºä½µç™¼
        logger.info("\næ¸¬è©¦2: å¤šæ•¸æ“šæºä½µç™¼æ“ä½œ")
        sources = []
        
        # å‰µå»ºå¤šå€‹æ•¸æ“šæº
        csv_file = Path('test_concurrent.csv')
        excel_file = Path('test_concurrent.xlsx')
        parquet_file = Path('test_concurrent.parquet')
        
        # æº–å‚™æ¸¬è©¦æ•¸æ“š
        test_df = pd.DataFrame({
            'id': range(100),
            'value': np.random.randn(100)
        })
        
        # å…ˆå‰µå»ºæª”æ¡ˆ
        test_df.to_csv(csv_file, index=False)
        test_df.to_excel(excel_file, index=False)
        test_df.to_parquet(parquet_file, index=False)
        
        # å‰µå»ºæ•¸æ“šæº
        csv_source = CSVSource.create_from_file(str(csv_file))
        excel_source = ExcelSource.create_from_file(str(excel_file))
        parquet_source = ParquetSource.create_from_file(str(parquet_file))
        
        sources = [csv_source, excel_source, parquet_source]
        
        # ä½µç™¼è®€å–æ‰€æœ‰æ•¸æ“šæº
        read_tasks = [source.read() for source in sources for _ in range(3)]  # æ¯å€‹æºè®€3æ¬¡
        read_results = await asyncio.gather(*read_tasks, return_exceptions=True)
        
        read_errors = sum(1 for r in read_results if isinstance(r, Exception))
        test2_success = read_errors == 0
        logger.info(f"å¤šæ•¸æ“šæºä½µç™¼æ¸¬è©¦: {'âœ… é€šé' if test2_success else 'âŒ å¤±æ•—'} ({read_errors} éŒ¯èª¤)")
        
        # æ¸…ç†
        for source in sources:
            await source.close()
        
        csv_file.unlink(missing_ok=True)
        excel_file.unlink(missing_ok=True)
        parquet_file.unlink(missing_ok=True)
        
        # æ¸¬è©¦3: æ¥µç«¯ä½µç™¼å£“åŠ›æ¸¬è©¦
        logger.info("\næ¸¬è©¦3: æ¥µç«¯ä½µç™¼å£“åŠ›æ¸¬è©¦")
        # stress_db = DuckDBSource.create_memory_db()
        stress_db = DuckDBSource.create_file_db(db_path)
        
        # å‰µå»ºæ¸¬è©¦è¡¨
        await stress_db.create_table('stress_test', {
            'id': 'INTEGER',
            'thread_id': 'INTEGER',
            'timestamp': 'TIMESTAMP',
            'data': 'VARCHAR'
        })
        
        # 50å€‹ä½µç™¼å¯«å…¥
        import threading
        write_tasks = []
        for i in range(50):
            thread_id = threading.current_thread().ident
            data = pd.DataFrame({
                'id': [i],
                'thread_id': [thread_id],
                'timestamp': [pd.Timestamp.now()],
                'data': [f'Test data {i}']
            })
            task = stress_db.write(data, table_name='stress_test', mode='append')
            write_tasks.append(task)
        
        write_results = await asyncio.gather(*write_tasks, return_exceptions=True)
        write_errors = sum(1 for r in write_results if isinstance(r, Exception))
        
        # é©—è­‰å¯«å…¥çµæœ
        count_result = await stress_db.read("SELECT COUNT(*) as cnt FROM stress_test")
        actual_count = count_result['cnt'].iloc[0] if not count_result.empty else 0
        
        test3_success = write_errors == 0 and actual_count == 50
        logger.info(f"å£“åŠ›æ¸¬è©¦: {'âœ… é€šé' if test3_success else 'âŒ å¤±æ•—'} ")
        logger.info(f"  - å¯«å…¥éŒ¯èª¤: {write_errors}")
        logger.info(f"  - å¯¦éš›å¯«å…¥: {actual_count}/50")
        
        await stress_db.close()
        
        # ç¸½çµ
        all_success = test1_success and test2_success and test3_success
        logger.info(f"\næ­»é–é é˜²æ¸¬è©¦ç¸½çµ: {'âœ… å…¨éƒ¨é€šé' if all_success else 'âŒ æœ‰æ¸¬è©¦å¤±æ•—'}")
        Path(db_path).unlink()
        
        return all_success
        
    except Exception as e:
        logger.error(f"æ­»é–é é˜²æ¸¬è©¦ç•°å¸¸: {str(e)}")
        import traceback
        logger.error(traceback.format_exc())
        return False


async def test_resource_cleanup():
    """æ¸¬è©¦è³‡æºæ¸…ç†æ©Ÿåˆ¶"""
    logger.info("=== æ¸¬è©¦è³‡æºæ¸…ç† ===")
    
    try:
        # æ¸¬è©¦ç·šç¨‹æ± æ¸…ç†
        logger.info("æ¸¬è©¦ç·šç¨‹æ± æ¸…ç†æ©Ÿåˆ¶")
        
        # å‰µå»ºå¤šå€‹æ•¸æ“šæº
        sources = []
        for i in range(5):
            csv_file = Path(f'cleanup_test_{i}.csv')
            pd.DataFrame({'data': [i]}).to_csv(csv_file, index=False)
            source = CSVSource.create_from_file(str(csv_file))
            sources.append((source, csv_file))
        
        # åŸ·è¡Œæ“ä½œ
        for source, _ in sources:
            await source.read()
        
        # é—œé–‰æ‰€æœ‰æ•¸æ“šæº
        for source, file_path in sources:
            await source.close()
            file_path.unlink(missing_ok=True)
        
        # æ‰‹å‹•è§¸ç™¼æ¸…ç†
        DataSourceFactory._cleanup_all_executors()
        
        logger.info("âœ… è³‡æºæ¸…ç†æ¸¬è©¦é€šé")
        return True
        
    except Exception as e:
        logger.error(f"è³‡æºæ¸…ç†æ¸¬è©¦å¤±æ•—: {str(e)}")
        return False


async def main():
    """é‹è¡Œæ‰€æœ‰æ¸¬è©¦"""
    logger.info("é–‹å§‹æ•¸æ“šæºæ¨¡çµ„æ¸¬è©¦")
    logger.info("=" * 60)
    
    test_results = []
    
    # é‹è¡Œå„é …æ¸¬è©¦
    tests = [
        ("Excelæ•¸æ“šæº", test_excel_source),
        ("CSVæ•¸æ“šæº", test_csv_source),
        ("Parquetæ•¸æ“šæº", test_parquet_source),
        ("DuckDBæ•¸æ“šæº", test_duckdb_source),
        ("æ•¸æ“šé·ç§»", test_data_migration),
        ("ä½µç™¼æ“ä½œ", test_concurrent_operations),
        ("ç·šç¨‹å®‰å…¨æ€§", test_thread_safety),
        ("æ­»é–é é˜²", test_deadlock_prevention),
        ("è³‡æºæ¸…ç†", test_resource_cleanup),
    ]
    
    for test_name, test_func in tests:
        try:
            logger.info(f"\né–‹å§‹æ¸¬è©¦: {test_name}")
            result = await test_func()
            test_results.append((test_name, result))
        except Exception as e:
            logger.error(f"{test_name}ç•°å¸¸: {str(e)}")
            test_results.append((test_name, False))
        
        logger.info("")  # ç©ºè¡Œåˆ†éš”
    
    # ç¸½çµæ¸¬è©¦çµæœ
    logger.info("=" * 60)
    logger.info("æ¸¬è©¦çµæœç¸½çµ:")
    passed = sum(1 for _, result in test_results if result)
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
        logger.info(f"{test_name:20}: {status}")
    
    logger.info(f"\nç¸½è¨ˆ: {passed}/{total} æ¸¬è©¦é€šé")
    logger.info(f"æˆåŠŸç‡: {(passed/total*100):.1f}%")
    
    if passed == total:
        logger.info("\nğŸ‰ æ‰€æœ‰æ¸¬è©¦é€šéï¼æ•¸æ“šæºæ¨¡çµ„é‹è¡Œæ­£å¸¸ã€‚")
    else:
        logger.warning(f"\nâš ï¸ æœ‰ {total-passed} é …æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥éŒ¯èª¤è¨Šæ¯ã€‚")
    
    # æ¸…ç†å…¨å±€ç·šç¨‹æ± 
    logger.info("\næ¸…ç†è³‡æº...")
    DataSourceFactory._cleanup_all_executors()
    
    return passed == total


if __name__ == "__main__":
    # é‹è¡Œæ¸¬è©¦
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
