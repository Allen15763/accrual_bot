"""
SPX Pipeline Checkpoint ç³»çµ±
è§£æ±ºæ¸¬è©¦æ™‚æ¯æ¬¡éƒ½è¦é‡è·‘è€—æ™‚æ­¥é©Ÿçš„å•é¡Œ

åŠŸèƒ½:
1. å„²å­˜ pipeline åŸ·è¡Œçš„ä¸­é–“ç‹€æ…‹
2. å¾æŒ‡å®šæ­¥é©Ÿæ¢å¾©åŸ·è¡Œ
3. å¿«é€Ÿæ¸¬è©¦å¾ŒçºŒæ­¥é©Ÿ

ä½¿ç”¨æ–¹å¼:
    # é¦–æ¬¡åŸ·è¡Œ - è‡ªå‹•å„²å­˜ checkpoint
    result = await execute_with_checkpoint(file_paths, 202509)
    
    # å¾ç‰¹å®šæ­¥é©Ÿæ¢å¾©
    result = await resume_from_step(
        checkpoint_name="SPX_202509_after_Filter_SPX_Products",
        start_from="Add_Columns"
    )
"""
import sys
import os
import time
from pathlib import Path

# æ·»åŠ æ¨¡çµ„è·¯å¾‘
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

import json
from pathlib import Path
from typing import Dict, Any, Optional, List
from datetime import datetime
# TEST NEW MODULE
import asyncio
import pandas as pd

from accrual_bot.core.pipeline.context import ProcessingContext
from accrual_bot.core.pipeline import Pipeline


class CheckpointManager:
    """Pipeline Checkpoint ç®¡ç†å™¨"""
    
    def __init__(self, checkpoint_dir: str = "./checkpoints"):
        """
        åˆå§‹åŒ– Checkpoint ç®¡ç†å™¨
        
        Args:
            checkpoint_dir: checkpoint å„²å­˜ç›®éŒ„
        """
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
    
    def save_checkpoint(
        self,
        context: ProcessingContext,
        step_name: str,
        metadata: Optional[Dict] = None
    ) -> str:
        """
        å„²å­˜ checkpoint
        
        Args:
            context: è™•ç†ä¸Šä¸‹æ–‡
            step_name: æ­¥é©Ÿåç¨±
            metadata: é¡å¤–çš„å…ƒæ•¸æ“š
            
        Returns:
            str: checkpoint åç¨±
        """
        # ç”Ÿæˆ checkpoint åç¨±
        entity_type = context.metadata.entity_type or "unknown"
        processing_date = context.metadata.processing_date or "unknown"
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        checkpoint_name = f"{entity_type}_{processing_date}_after_{step_name}"
        checkpoint_path = self.checkpoint_dir / checkpoint_name
        checkpoint_path.mkdir(parents=True, exist_ok=True)
        
        # å„²å­˜ä¸»æ•¸æ“š
        if context.data is not None and not context.data.empty:
            data_path = checkpoint_path / "data.parquet"
            context.data.to_parquet(data_path, index=False)
        
        # å„²å­˜è¼”åŠ©æ•¸æ“š
        aux_data_dir = checkpoint_path / "auxiliary_data"
        aux_data_dir.mkdir(exist_ok=True)
        
        for aux_name in context.list_auxiliary_data():
            aux_data = context.get_auxiliary_data(aux_name)
            if aux_data is not None and not aux_data.empty:
                aux_path = aux_data_dir / f"{aux_name}.parquet"
                try:
                    if 'ops_validation' in aux_name:
                        aux_data['discount'] = aux_data['discount'].astype(str)
                    else:
                        aux_data.to_parquet(aux_path, index=False)
                except Exception as err:
                    print(f"ERROR exporting parquet on {aux_name}")
        
        # å„²å­˜è®Šæ•¸å’Œå…ƒæ•¸æ“š
        checkpoint_info = {
            'step_name': step_name,
            'entity_type': context.metadata.entity_type,
            'processing_date': context.metadata.processing_date,
            'processing_type': context.metadata.processing_type,
            'variables': context._variables,
            'warnings': context.warnings,
            'errors': context.errors,
            'timestamp': timestamp,
            'auxiliary_data_list': context.list_auxiliary_data(),
            'metadata': metadata or {}
        }
        
        with open(checkpoint_path / "checkpoint_info.json", 'w', encoding='utf-8') as f:
            json.dump(checkpoint_info, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"âœ… Checkpoint å·²å„²å­˜: {checkpoint_name}")
        return checkpoint_name
    
    def load_checkpoint(self, checkpoint_name: str) -> ProcessingContext:
        """
        è¼‰å…¥ checkpoint
        
        Args:
            checkpoint_name: checkpoint åç¨±
            
        Returns:
            ProcessingContext: æ¢å¾©çš„ä¸Šä¸‹æ–‡
        """
        checkpoint_path = self.checkpoint_dir / checkpoint_name
        
        if not checkpoint_path.exists():
            raise FileNotFoundError(f"Checkpoint ä¸å­˜åœ¨: {checkpoint_name}")
        
        # è¼‰å…¥å…ƒæ•¸æ“š
        with open(checkpoint_path / "checkpoint_info.json", 'r', encoding='utf-8') as f:
            info = json.load(f)
        
        # è¼‰å…¥ä¸»æ•¸æ“š
        data_path = checkpoint_path / "data.parquet"
        if data_path.exists():
            data = pd.read_parquet(data_path)
        else:
            data = pd.DataFrame()
        
        # å‰µå»ºä¸Šä¸‹æ–‡
        context = ProcessingContext(
            data=data,
            entity_type=info['entity_type'],
            processing_date=info['processing_date'],
            processing_type=info['processing_type']
        )
        
        # æ¢å¾©è®Šæ•¸
        for key, value in info['variables'].items():
            context.set_variable(key, value)
        
        # æ¢å¾©è¼”åŠ©æ•¸æ“š
        aux_data_dir = checkpoint_path / "auxiliary_data"
        if aux_data_dir.exists():
            for aux_file in aux_data_dir.glob("*.parquet"):
                aux_name = aux_file.stem
                aux_data = pd.read_parquet(aux_file)
                context.add_auxiliary_data(aux_name, aux_data)
        
        print(f"âœ… Checkpoint å·²è¼‰å…¥: {checkpoint_name}")
        print(f"   - ä¸»æ•¸æ“š: {len(context.data)} è¡Œ")
        print(f"   - è¼”åŠ©æ•¸æ“š: {len(context.list_auxiliary_data())} å€‹")
        print(f"   - è®Šæ•¸: {len(context._variables)} å€‹")
        
        return context
    
    def list_checkpoints(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„ checkpoint"""
        checkpoints = []
        
        for checkpoint_path in self.checkpoint_dir.iterdir():
            if checkpoint_path.is_dir():
                info_file = checkpoint_path / "checkpoint_info.json"
                if info_file.exists():
                    with open(info_file, 'r', encoding='utf-8') as f:
                        info = json.load(f)
                    checkpoints.append({
                        'name': checkpoint_path.name,
                        'step': info['step_name'],
                        'date': info['processing_date'],
                        'timestamp': info['timestamp']
                    })
        
        return sorted(checkpoints, key=lambda x: x['timestamp'], reverse=True)
    
    def delete_checkpoint(self, checkpoint_name: str):
        """åˆªé™¤æŒ‡å®šçš„ checkpoint"""
        checkpoint_path = self.checkpoint_dir / checkpoint_name
        if checkpoint_path.exists():
            import shutil
            shutil.rmtree(checkpoint_path)
            print(f"âœ… Checkpoint å·²åˆªé™¤: {checkpoint_name}")


class PipelineWithCheckpoint:
    """
    å¸¶ Checkpoint åŠŸèƒ½çš„ Pipeline åŸ·è¡Œå™¨
    """
    
    def __init__(self, pipeline: Pipeline, checkpoint_manager: CheckpointManager):
        self.pipeline = pipeline
        self.checkpoint_manager = checkpoint_manager
    
    async def execute_with_checkpoint(
        self,
        context: ProcessingContext,
        save_after_each_step: bool = True,
        start_from_step: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        åŸ·è¡Œ Pipeline ä¸¦è‡ªå‹•å„²å­˜ checkpoint
        
        Args:
            context: è™•ç†ä¸Šä¸‹æ–‡
            save_after_each_step: æ˜¯å¦åœ¨æ¯å€‹æ­¥é©Ÿå¾Œå„²å­˜ checkpoint
            start_from_step: å¾å“ªå€‹æ­¥é©Ÿé–‹å§‹åŸ·è¡Œ (None = å¾é ­é–‹å§‹)
            
        Returns:
            Dict: åŸ·è¡Œçµæœ
        """
        # æ‰¾åˆ°èµ·å§‹æ­¥é©Ÿçš„ç´¢å¼•
        start_index = 0
        if start_from_step:
            for i, step in enumerate(self.pipeline.steps):
                if step.name == start_from_step:
                    start_index = i
                    print(f"ğŸ”„ å¾æ­¥é©Ÿ '{start_from_step}' é–‹å§‹åŸ·è¡Œ (è·³éå‰ {i} å€‹æ­¥é©Ÿ)")
                    break
            else:
                raise ValueError(f"æ‰¾ä¸åˆ°æ­¥é©Ÿ: {start_from_step}")
        
        # åŸ·è¡Œæ­¥é©Ÿ
        results = []
        for i, step in enumerate(self.pipeline.steps[start_index:], start=start_index):
            print(f"\n{'='*60}")
            print(f"åŸ·è¡Œæ­¥é©Ÿ {i+1}/{len(self.pipeline.steps)}: {step.name}")
            print(f"{'='*60}")
            
            # åŸ·è¡Œæ­¥é©Ÿ
            result = await step.execute(context)
            results.append(result)
            
            # å„²å­˜ checkpoint
            if save_after_each_step and result.is_success:
                self.checkpoint_manager.save_checkpoint(
                    context=context,
                    step_name=step.name,
                    metadata={
                        'step_index': i,
                        'step_status': result.status.value,
                        'step_message': result.message
                    }
                )
            
            # å¦‚æœå¤±æ•—ä¸”è¨­å®šç‚ºé‡éŒ¯å³åœ
            if not result.is_success and self.pipeline.config.stop_on_error:
                print(f"âŒ æ­¥é©Ÿå¤±æ•—,åœæ­¢åŸ·è¡Œ: {result.message}")
                break
        
        # å½™ç¸½çµæœ
        successful = sum(1 for r in results if r.is_success)
        failed = sum(1 for r in results if not r.is_success and not r.is_skipped)
        # skipped = sum(1 for r in results if r.is_skipped)
        
        return {
            'success': failed == 0,
            'total_steps': len(results),
            'successful_steps': successful,
            'failed_steps': failed,
            # 'skipped_steps': skipped,
            'results': results,  # List[StepResult]
            'context': context
        }


# =============================================================================
# ä¾¿æ·å‡½æ•¸
# =============================================================================

async def execute_with_checkpoint(
    file_paths: Dict[str, str],
    processing_date: int,
    checkpoint_dir: str = "./checkpoints",
    save_checkpoints: bool = True,
    processing_type: str = 'PO'
) -> Dict[str, Any]:
    """
    åŸ·è¡Œå®Œæ•´ pipeline ä¸¦è‡ªå‹•å„²å­˜ checkpoint
    
    Args:
        file_paths: æ–‡ä»¶è·¯å¾‘å­—å…¸
        processing_date: è™•ç†æ—¥æœŸ
        checkpoint_dir: checkpoint å„²å­˜ç›®éŒ„
        save_checkpoints: æ˜¯å¦å„²å­˜ checkpoint
        
    Returns:
        Dict: åŸ·è¡Œçµæœ
    """
    from accrual_bot.core.pipeline.steps.spx_po_steps import create_spx_po_complete_pipeline  # æ›¿æ›æˆå¯¦éš›è·¯å¾‘
    
    # å‰µå»º pipeline å’Œ checkpoint manager
    pipeline = create_spx_po_complete_pipeline(file_paths)
    checkpoint_manager = CheckpointManager(checkpoint_dir)
    
    # å‰µå»ºä¸Šä¸‹æ–‡
    context = ProcessingContext(
        data=pd.DataFrame(),
        entity_type="SPX",
        processing_date=processing_date,
        processing_type=processing_type
    )
    
    # åŸ·è¡Œ
    executor = PipelineWithCheckpoint(pipeline, checkpoint_manager)
    result = await executor.execute_with_checkpoint(
        context=context,
        save_after_each_step=save_checkpoints
    )
    
    return result

async def execute_pr_with_checkpoint(
    file_paths: Dict[str, str],
    processing_date: int,
    checkpoint_dir: str = "./checkpoints",
    save_checkpoints: bool = True,
    processing_type: str = 'PR'
) -> Dict[str, Any]:
    """
    åŸ·è¡Œå®Œæ•´ pipeline ä¸¦è‡ªå‹•å„²å­˜ checkpoint
    
    Args:
        file_paths: æ–‡ä»¶è·¯å¾‘å­—å…¸
        processing_date: è™•ç†æ—¥æœŸ
        checkpoint_dir: checkpoint å„²å­˜ç›®éŒ„
        save_checkpoints: æ˜¯å¦å„²å­˜ checkpoint
        
    Returns:
        Dict: åŸ·è¡Œçµæœ
    """
    from accrual_bot.core.pipeline.steps.spx_steps import create_spx_pr_complete_pipeline  # æ›¿æ›æˆå¯¦éš›è·¯å¾‘
    
    # å‰µå»º pipeline å’Œ checkpoint manager
    pipeline = create_spx_pr_complete_pipeline(file_paths)
    checkpoint_manager = CheckpointManager(checkpoint_dir)
    
    # å‰µå»ºä¸Šä¸‹æ–‡
    context = ProcessingContext(
        data=pd.DataFrame(),
        entity_type="SPX",
        processing_date=processing_date,
        processing_type=processing_type
    )
    
    # åŸ·è¡Œ
    executor = PipelineWithCheckpoint(pipeline, checkpoint_manager)
    result = await executor.execute_with_checkpoint(
        context=context,
        save_after_each_step=save_checkpoints
    )
    
    return result

async def execute_ppe_with_checkpoint(
    file_paths: str,
    processing_date: int,
    checkpoint_dir: str = "./checkpoints",
    save_checkpoints: bool = True
) -> Dict[str, Any]:
    """
    åŸ·è¡Œå®Œæ•´ pipeline ä¸¦è‡ªå‹•å„²å­˜ checkpoint
    
    Args:
        file_paths: æ–‡ä»¶è·¯å¾‘å­—ä¸²
        processing_date: è™•ç†æ—¥æœŸ
        checkpoint_dir: checkpoint å„²å­˜ç›®éŒ„
        save_checkpoints: æ˜¯å¦å„²å­˜ checkpoint
        
    Returns:
        Dict: åŸ·è¡Œçµæœ
    """
    from accrual_bot.core.pipeline.steps.spx_po_steps import create_ppe_pipeline  # æ›¿æ›æˆå¯¦éš›è·¯å¾‘
    
    # å‰µå»º pipeline å’Œ checkpoint manager
    pipeline = create_ppe_pipeline(file_paths, processing_date)
    checkpoint_manager = CheckpointManager(checkpoint_dir)
    
    # å‰µå»ºä¸Šä¸‹æ–‡
    context = ProcessingContext(
        data=pd.DataFrame(),
        entity_type="SPX",
        processing_date=processing_date,
        processing_type="PO"
    )
    
    # åŸ·è¡Œ
    executor = PipelineWithCheckpoint(pipeline, checkpoint_manager)
    result = await executor.execute_with_checkpoint(
        context=context,
        save_after_each_step=save_checkpoints
    )
    
    return result


async def resume_from_step(
    checkpoint_name: str,
    start_from_step: str,
    file_paths: Optional[Dict[str, str]] = None,
    checkpoint_dir: str = "./checkpoints"
) -> Dict[str, Any]:
    """
    å¾ checkpoint æ¢å¾©ä¸¦å¾æŒ‡å®šæ­¥é©Ÿé–‹å§‹åŸ·è¡Œ
    
    Args:
        checkpoint_name: checkpoint åç¨±
        start_from_step: å¾å“ªå€‹æ­¥é©Ÿé–‹å§‹
        file_paths: æ–‡ä»¶è·¯å¾‘ (å¦‚æœéœ€è¦é‡å»º pipeline)
        checkpoint_dir: checkpoint ç›®éŒ„
        
    Returns:
        Dict: åŸ·è¡Œçµæœ
    """
    from accrual_bot.core.pipeline.steps.spx_po_steps import create_spx_po_complete_pipeline  # æ›¿æ›æˆå¯¦éš›è·¯å¾‘
    
    # è¼‰å…¥ checkpoint
    checkpoint_manager = CheckpointManager(checkpoint_dir)
    context = checkpoint_manager.load_checkpoint(checkpoint_name)
    
    # é‡å»º pipeline (ä½¿ç”¨åŸå§‹ file_paths æˆ–å¾ context ç²å–)
    if file_paths is None:
        # å˜—è©¦å¾ context ä¸­ç²å–æ–‡ä»¶è·¯å¾‘
        file_paths = context.get_variable('file_paths', {})
        if not file_paths:
            raise ValueError("ç„¡æ³•ç²å–æ–‡ä»¶è·¯å¾‘,è«‹æä¾› file_paths åƒæ•¸")
    
    pipeline = create_spx_po_complete_pipeline(file_paths)
    
    # åŸ·è¡Œ
    executor = PipelineWithCheckpoint(pipeline, checkpoint_manager)
    result = await executor.execute_with_checkpoint(
        context=context,
        save_after_each_step=True,
        start_from_step=start_from_step
    )
    
    return result


async def quick_test_step(
    checkpoint_name: str,
    step_to_test: str,
    checkpoint_dir: str = "./checkpoints"
) -> Dict[str, Any]:
    """
    å¿«é€Ÿæ¸¬è©¦å–®ä¸€æ­¥é©Ÿ (å¾ä¸Šä¸€å€‹ checkpoint æ¢å¾©)
    
    Args:
        checkpoint_name: checkpoint åç¨±
        step_to_test: è¦æ¸¬è©¦çš„æ­¥é©Ÿåç¨±
        checkpoint_dir: checkpoint ç›®éŒ„
        
    Returns:
        Dict: åŸ·è¡Œçµæœ
    """
    return await resume_from_step(
        checkpoint_name=checkpoint_name,
        start_from_step=step_to_test,
        checkpoint_dir=checkpoint_dir
    )


# =============================================================================
# ä½¿ç”¨ç¯„ä¾‹
# =============================================================================

async def example_usage():
    """ä½¿ç”¨ç¯„ä¾‹"""
    
    file_paths = {
        'raw_po': {
            'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202509\202509_purchase_order.csv",
            'params': {'encoding': 'utf-8', 
                       'sep': ',', 
                       'dtype': str, 
                       'keep_default_na': False, 
                       'na_values': ['']
                       }
        },
        'previous': {
            'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202509\202508_PO_FN.xlsx",
            'params': {'sheet_name': 0, 'header': 0, 'dtype': str, }
        },
        'procurement_po': {
            'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202509\202509_PO_PQ.xlsx",
            'params': {'dtype': str, }
        },
        'ap_invoice': {
            'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202509\202509_AP_Invoice_Match_Monitoring_Ext.xlsx",
            'params': {}
        },
        'previous_pr': {
            'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202509\202508_PR_FN.xlsx",
            'params': {'dtype': str, }
        },
        'procurement_pr': {
            'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202509\202509_PR_PQ.xlsx",
            'params': {'dtype': str, }
        },
        'ops_validation': {
            'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202509\SPXæ™ºå–æ«ƒåŠç¹³è²»æ©Ÿé©—æ”¶æ˜ç´°(For FN)_2509.xlsx",
            'params': {
                'sheet_name': 'æ™ºå–æ«ƒé©—æ”¶æ˜ç´°',
                'header': 1,  # ç¬¬äºŒè¡Œä½œç‚ºè¡¨é ­
                'usecols': 'A:AE',
                # 'dtype': str, 
            }
        }
    }
    
    # ========== æƒ…å¢ƒ 1: é¦–æ¬¡åŸ·è¡Œ,è‡ªå‹•å„²å­˜ checkpoint ==========
    print("æƒ…å¢ƒ 1: é¦–æ¬¡åŸ·è¡Œ")
    result = await execute_with_checkpoint(
        file_paths=file_paths,
        processing_date=202509,
        save_checkpoints=True
    )
    
    # ========== æƒ…å¢ƒ 2: æŸ¥çœ‹å¯ç”¨çš„ checkpoint ==========
    print("\næƒ…å¢ƒ 2: æŸ¥çœ‹ checkpoints")
    checkpoint_manager = CheckpointManager()
    checkpoints = checkpoint_manager.list_checkpoints()
    for cp in checkpoints:
        print(f"  - {cp['name']} (æ­¥é©Ÿ: {cp['step']}, æ™‚é–“: {cp['timestamp']})")
    
    # ========== æƒ…å¢ƒ 3: å¾ç‰¹å®šæ­¥é©Ÿæ¢å¾© (å‰é¢æ­¥é©Ÿå·²æ¸¬è©¦å®Œæˆ) ==========
    print("\næƒ…å¢ƒ 3: å¾ Add_Columns æ­¥é©Ÿé–‹å§‹")
    result = await resume_from_step(
        checkpoint_name="SPX_202509_after_Filter_SPX_Products",
        start_from_step="Add_Columns",
        file_paths=file_paths  # å¯é¸,å¦‚æœ checkpoint ä¸­æ²’æœ‰
    )
    
    # ========== æƒ…å¢ƒ 4: å¿«é€Ÿæ¸¬è©¦æŸå€‹æ­¥é©Ÿ ==========
    print("\næƒ…å¢ƒ 4: å¿«é€Ÿæ¸¬è©¦ AP Invoice Integration")
    result = await quick_test_step(
        checkpoint_name="SPX_202509_after_Add_Columns",
        step_to_test="Integrate_AP_Invoice"
    )
    
    # ========== æƒ…å¢ƒ 5: åˆªé™¤èˆŠçš„ checkpoint ==========
    print("\næƒ…å¢ƒ 5: æ¸…ç†èˆŠ checkpoints")
    checkpoint_manager.delete_checkpoint("SPX_202509_after_Load_All_Data")


if __name__ == "__main__":
    import warnings
    warnings.filterwarnings('ignore')
    # asyncio.run(example_usage())

    file_paths = {
        'raw_po': {
            'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202509\202509_purchase_order.csv",
            'params': {'encoding': 'utf-8', 
                       'sep': ',', 
                       'dtype': str, 
                       'keep_default_na': False, 
                       'na_values': ['']
                       }
        },
        'previous': {
            'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202509\202508_PO_FN.xlsx",
            'params': {'sheet_name': 0, 'header': 0, 'dtype': str, }
        },
        'procurement_po': {
            'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202509\202509_PO_PQ.xlsx",
            'params': {'dtype': str, }
        },
        'ap_invoice': {
            'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202509\202509_AP_Invoice_Match_Monitoring_Ext.xlsx",
            'params': {}
        },
        'previous_pr': {
            'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202509\202508_PR_FN.xlsx",
            'params': {'dtype': str, }
        },
        'procurement_pr': {
            'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202509\202509_PR_PQ.xlsx",
            'params': {'dtype': str, }
        },
        'ops_validation': {
            'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202509\SPXæ™ºå–æ«ƒåŠç¹³è²»æ©Ÿé©—æ”¶æ˜ç´°(For FN)_2509.xlsx",
            'params': {
                'sheet_name': 'æ™ºå–æ«ƒé©—æ”¶æ˜ç´°',
                'header': 1,  # ç¬¬äºŒè¡Œä½œç‚ºè¡¨é ­
                'usecols': 'A:AE',
                # 'dtype': str, 
            }
        }
    }

    # file_paths = {
    #     'raw_po': {
    #         'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202508\SPXæœªçµFor æ©Ÿå™¨äºº\202508_purchase_order.csv",
    #         'params': {'encoding': 'utf-8', 
    #                    'sep': ',', 
    #                    'dtype': str, 
    #                    'keep_default_na': False, 
    #                    'na_values': ['']
    #                    }
    #     },
    #     'previous': {
    #         'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202508\SPXæœªçµFor æ©Ÿå™¨äºº\202507_PO_FN.xlsx",
    #         'params': {'sheet_name': 0, 'header': 0, 'dtype': str, }
    #     },
    #     'procurement_po': {
    #         'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202508\SPXæœªçµFor æ©Ÿå™¨äºº\202508_PO_PQ.xlsx",
    #         'params': {'dtype': str, }
    #     },
    #     'ap_invoice': {
    #         'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202508\SPXæœªçµFor æ©Ÿå™¨äºº\AP_Invoice_Match_Monitoring_Ext (NEW).xlsx",
    #         'params': {}
    #     },
    #     'previous_pr': {
    #         'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202508\SPXæœªçµFor æ©Ÿå™¨äºº\202507_PR_FN.xlsx",
    #         'params': {'dtype': str, }
    #     },
    #     'procurement_pr': {
    #         'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202508\SPXæœªçµFor æ©Ÿå™¨äºº\202508_PR_PQ.xlsx",
    #         'params': {'dtype': str, }
    #     },
    #     'ops_validation': {
    #         'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202508\SPXæœªçµFor æ©Ÿå™¨äºº\SPXæ™ºå–æ«ƒåŠç¹³è²»æ©Ÿé©—æ”¶æ˜ç´°(For FN)_2508_ä¿®å¾©.xlsx",
    #         'params': {
    #             'sheet_name': 'æ™ºå–æ«ƒé©—æ”¶æ˜ç´°',
    #             'header': 1,  # ç¬¬äºŒè¡Œä½œç‚ºè¡¨é ­
    #             'usecols': 'A:AE',
    #             # 'dtype': str, 
    #         }
    #     }
    # }
    
    # Run all steps
    # result = asyncio.run(execute_with_checkpoint(
    #     file_paths=file_paths,
    #     processing_date=202509,
    #     save_checkpoints=True
    # ))

    # Start from specific point
    # result = asyncio.run(resume_from_step(
    #     checkpoint_name="SPX_202509_after_Filter_SPX_Products",    # checkpointè³‡æ–™å¤¾è·¯å¾‘åç¨±
    #     start_from_step="Add_Columns",
    #     # checkpoint_name="SPX_202509_after_Process_Dates",    # checkpointè³‡æ–™å¤¾è·¯å¾‘åç¨±
    #     # start_from_step="Integrate_Closing_List",
    #     file_paths=file_paths  # å¯é¸,å¦‚æœ checkpoint ä¸­æ²’æœ‰
    # ))

    # å¾ç‰¹å®šæ­¥é©Ÿé–‹å§‹ï¼Œè·Ÿresume_from_stepé¡ä¼¼
    # result = asyncio.run(quick_test_step(
    #     checkpoint_name="SPX_202509_after_Add_Columns",
    #     step_to_test="Integrate_AP_Invoice"
    # ))

    # Run PPE steps
    # result = asyncio.run(execute_ppe_with_checkpoint(
    #     file_paths=r'G:\å…±ç”¨é›²ç«¯ç¡¬ç¢Ÿ\INT_TWN_SEA_FN_Shared_Resources\00_Temp_Internal_share\SPX\ç§Ÿé‡‘\SPXç§Ÿé‡‘åˆç´„æ­¸æª”æ¸…å–®åŠåŒ¯æ¬¾ç‹€æ…‹_marge1.xlsx',
    #     processing_date=202509,
    #     save_checkpoints=True
    # ))

    # Run PR
    file_paths_pr = {
        'raw_pr': {
            'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202509\202509_purchase_request.xlsx",
            'params': {'encoding': 'utf-8', 
                       'sep': ',', 
                       'dtype': str, 
                       'keep_default_na': False, 
                       'na_values': ['']
                       }
        },
        'previous_pr': {
            'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202509\202508_PR_FN.xlsx",  # xxx_æ”¹æ¬„åï¼Œæš«ä¸éœ€è¦
            'params': {'dtype': str, }
        },
        'procurement_pr': {
            'path': r"C:\SEA\Accrual\prpo_bot\resources\SPXæœªçµæ¨¡çµ„\raw_202509\202509_PR_PQ.xlsx",
            'params': {'dtype': str, }
        },

    }
    result = asyncio.run(execute_pr_with_checkpoint(
        file_paths=file_paths_pr,
        processing_date=202509,
        save_checkpoints=False
    ))
    
    print(1)
